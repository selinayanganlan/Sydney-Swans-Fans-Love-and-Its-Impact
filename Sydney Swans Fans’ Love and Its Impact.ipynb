{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U32UWzeW_Ckx"
   },
   "source": [
    "# 2 Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "executionInfo": {
     "elapsed": 2024,
     "status": "ok",
     "timestamp": 1618474169812,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "JTCTB68nEosZ",
    "outputId": "5bc6ac30-dce9-4f47-db34-0da4208072ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Apps\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q11_id</th>\n",
       "      <th>q12_age_int</th>\n",
       "      <th>q13_age_attd</th>\n",
       "      <th>q14_no_game</th>\n",
       "      <th>q15_team_sup</th>\n",
       "      <th>q16_attd_who</th>\n",
       "      <th>mar28</th>\n",
       "      <th>apr17</th>\n",
       "      <th>may2</th>\n",
       "      <th>may9</th>\n",
       "      <th>may23</th>\n",
       "      <th>june6</th>\n",
       "      <th>june27</th>\n",
       "      <th>july4</th>\n",
       "      <th>july24</th>\n",
       "      <th>july31</th>\n",
       "      <th>aug15</th>\n",
       "      <th>q18_game_last</th>\n",
       "      <th>q19_sport_fx</th>\n",
       "      <th>q21</th>\n",
       "      <th>q22</th>\n",
       "      <th>q23</th>\n",
       "      <th>q24</th>\n",
       "      <th>q25</th>\n",
       "      <th>q26</th>\n",
       "      <th>q27</th>\n",
       "      <th>q28</th>\n",
       "      <th>q29</th>\n",
       "      <th>q210</th>\n",
       "      <th>q211</th>\n",
       "      <th>q3</th>\n",
       "      <th>q41</th>\n",
       "      <th>q42</th>\n",
       "      <th>q43</th>\n",
       "      <th>q44</th>\n",
       "      <th>q45</th>\n",
       "      <th>q46</th>\n",
       "      <th>q51</th>\n",
       "      <th>q52</th>\n",
       "      <th>q53</th>\n",
       "      <th>q54</th>\n",
       "      <th>q55</th>\n",
       "      <th>q56</th>\n",
       "      <th>q57</th>\n",
       "      <th>q58</th>\n",
       "      <th>q61</th>\n",
       "      <th>q62</th>\n",
       "      <th>q63</th>\n",
       "      <th>q64</th>\n",
       "      <th>q65</th>\n",
       "      <th>q71</th>\n",
       "      <th>q711</th>\n",
       "      <th>q712</th>\n",
       "      <th>q72</th>\n",
       "      <th>q721</th>\n",
       "      <th>q722</th>\n",
       "      <th>q723</th>\n",
       "      <th>q73</th>\n",
       "      <th>q731</th>\n",
       "      <th>q732</th>\n",
       "      <th>q74</th>\n",
       "      <th>q741</th>\n",
       "      <th>q742</th>\n",
       "      <th>q75</th>\n",
       "      <th>q751</th>\n",
       "      <th>q81</th>\n",
       "      <th>q82</th>\n",
       "      <th>q83</th>\n",
       "      <th>q84</th>\n",
       "      <th>q85</th>\n",
       "      <th>q91</th>\n",
       "      <th>q911</th>\n",
       "      <th>q912</th>\n",
       "      <th>q92</th>\n",
       "      <th>q921</th>\n",
       "      <th>q922</th>\n",
       "      <th>q923</th>\n",
       "      <th>q924</th>\n",
       "      <th>q93</th>\n",
       "      <th>q931</th>\n",
       "      <th>q932</th>\n",
       "      <th>q933</th>\n",
       "      <th>q934</th>\n",
       "      <th>q94</th>\n",
       "      <th>q941</th>\n",
       "      <th>q942</th>\n",
       "      <th>q943</th>\n",
       "      <th>q95</th>\n",
       "      <th>q951</th>\n",
       "      <th>q952</th>\n",
       "      <th>q953</th>\n",
       "      <th>q10a</th>\n",
       "      <th>q10b</th>\n",
       "      <th>q10c</th>\n",
       "      <th>q10d</th>\n",
       "      <th>q10e</th>\n",
       "      <th>q10f</th>\n",
       "      <th>q11a</th>\n",
       "      <th>q11b</th>\n",
       "      <th>q11c</th>\n",
       "      <th>q11d</th>\n",
       "      <th>q11e</th>\n",
       "      <th>q11f</th>\n",
       "      <th>q11g</th>\n",
       "      <th>q12a</th>\n",
       "      <th>q12b</th>\n",
       "      <th>q12c</th>\n",
       "      <th>q12d</th>\n",
       "      <th>q131</th>\n",
       "      <th>q132</th>\n",
       "      <th>q133</th>\n",
       "      <th>q134</th>\n",
       "      <th>q135</th>\n",
       "      <th>q136</th>\n",
       "      <th>q137</th>\n",
       "      <th>q138</th>\n",
       "      <th>q139</th>\n",
       "      <th>q141</th>\n",
       "      <th>q142</th>\n",
       "      <th>q143</th>\n",
       "      <th>q144</th>\n",
       "      <th>q145</th>\n",
       "      <th>q146</th>\n",
       "      <th>q151</th>\n",
       "      <th>q152</th>\n",
       "      <th>q153</th>\n",
       "      <th>q154</th>\n",
       "      <th>q155</th>\n",
       "      <th>q156</th>\n",
       "      <th>q157</th>\n",
       "      <th>q158</th>\n",
       "      <th>q159</th>\n",
       "      <th>q1510</th>\n",
       "      <th>q16</th>\n",
       "      <th>q17</th>\n",
       "      <th>q18</th>\n",
       "      <th>q18b</th>\n",
       "      <th>q19</th>\n",
       "      <th>q20</th>\n",
       "      <th>q21_agev152</th>\n",
       "      <th>q22_sexv153</th>\n",
       "      <th>q23_postcode</th>\n",
       "      <th>q24a</th>\n",
       "      <th>q24b</th>\n",
       "      <th>q24c</th>\n",
       "      <th>q24d</th>\n",
       "      <th>q24e</th>\n",
       "      <th>q24f</th>\n",
       "      <th>q25_incomev161</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>968</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>733</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>389</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>924</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>762</td>\n",
       "      <td>37.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   q11_id  q12_age_int  q13_age_attd  q14_no_game  q15_team_sup  q16_attd_who  \\\n",
       "0     968          5.0          12.0          2.0           2.0           3.0   \n",
       "1     733         20.0          20.0          3.0           1.0           3.0   \n",
       "2     389         14.0          20.0          4.0           2.0           3.0   \n",
       "3     924         30.0          30.0          3.0           1.0           3.0   \n",
       "4     762         37.0          34.0          3.0           1.0           2.0   \n",
       "\n",
       "   mar28  apr17  may2  may9  may23  june6  june27  july4  july24  july31  \\\n",
       "0      1      0     1     1      0      0       0      1       1       1   \n",
       "1      0      0     0     1      1      1       1      1       1       1   \n",
       "2      1      1     1     1      1      1       1      1       1       1   \n",
       "3      1      1     1     1      1      0       0      0       0       0   \n",
       "4      1      1     1     1      1      1       1      1       1       1   \n",
       "\n",
       "   aug15  q18_game_last  q19_sport_fx  q21  q22  q23  q24  q25  q26  q27  q28  \\\n",
       "0      1            1.0           3.0    6    1    7  7.0  1.0  7.0    2    5   \n",
       "1      1            1.0           4.0    1    1    6  7.0  4.0  6.0    4    5   \n",
       "2      1            2.0           1.0    3    1    5  5.0  5.0  4.0    5    5   \n",
       "3      0            1.0           4.0    1    1    7  7.0  1.0  4.0    1    7   \n",
       "4      1            2.0           3.0    7    4    4  7.0  7.0  2.0    6    7   \n",
       "\n",
       "   q29  q210  q211   q3  q41  q42  q43  q44  q45  q46  q51  q52  q53  q54  \\\n",
       "0  7.0   2.0   2.0  5.0    2    2    5    6    2    1    2    4    5    5   \n",
       "1  4.0   1.0   7.0  4.0    5    4    7    7    6    5    5    5    7    7   \n",
       "2  5.0   4.0   4.0  7.0    4    3    4    6    6    5    7    5    7    7   \n",
       "3  7.0   1.0   4.0  4.0    1    1    1    2    1    1    1    1    4    4   \n",
       "4  7.0   2.0   7.0  7.0    7    4    7    6    7    4    7    7    7    7   \n",
       "\n",
       "   q55  q56  q57  q58  q61   q62   q63   q64  q65  q71  q711  q712  q72  q721  \\\n",
       "0    1    1    1    1   30  40.0  20.0  10.0    0    6   5.0   7.0    6   4.0   \n",
       "1    5    5    4    6   30  20.0  20.0  20.0   10    4   4.0   4.0    4   4.0   \n",
       "2    5    5    4    4   20  25.0  25.0  20.0   10    6   6.0   6.0    5   5.0   \n",
       "3    1    1    1    1   20  20.0  30.0  20.0   10    4   4.0   4.0    4   4.0   \n",
       "4    4    6    1    5   10  20.0  40.0  25.0    5    5   5.0   2.0    4   2.0   \n",
       "\n",
       "   q722  q723  q73  q731  q732  q74  q741  q742  q75  q751   q81   q82   q83  \\\n",
       "0   6.0   6.0    6   7.0   6.0    3   4.0   5.0    6   4.0  10.0  15.0  15.0   \n",
       "1   5.0   4.0    4   4.0   4.0    5   5.0   5.0    5   4.0  20.0  20.0  20.0   \n",
       "2   6.0   4.0    5   5.0   5.0    5   7.0   7.0    6   5.0  20.0  25.0  20.0   \n",
       "3   4.0   4.0    4   4.0   4.0    4   4.0   4.0    4   4.0  30.0  10.0  10.0   \n",
       "4   3.0   3.0    4   3.0   2.0    3   3.0   4.0    2   5.0  25.0   5.0  20.0   \n",
       "\n",
       "    q84   q85  q91  q911  q912  q92  q921  q922  q923  q924  q93  q931  q932  \\\n",
       "0  30.0  30.0    6   6.0   5.0    6   6.0   7.0   5.0   6.0    6   5.0   7.0   \n",
       "1  20.0  20.0    4   5.0   6.0    4   5.0   5.0   5.0   6.0    5   5.0   5.0   \n",
       "2  20.0  15.0    5   6.0   4.0    5   7.0   5.0   5.0   5.0    5   5.0   6.0   \n",
       "3  20.0  30.0    4   4.0   4.0    4   4.0   4.0   4.0   4.0    4   4.0   4.0   \n",
       "4  40.0  10.0    6   6.0   6.0    4   2.0   1.0   1.0   3.0    3   2.0   5.0   \n",
       "\n",
       "   q933  q934  q94  q941  q942  q943  q95  q951  q952  q953  q10a  q10b  q10c  \\\n",
       "0   6.0   5.0    6   6.0   6.0   6.0    7   7.0   7.0   7.0     2     3     5   \n",
       "1   4.0   5.0    5   6.0   6.0   6.0    5   6.0   6.0   6.0     5     6     6   \n",
       "2   4.0   5.0    6   6.0   7.0   5.0    5   7.0   5.0   6.0     7     7     7   \n",
       "3   4.0   4.0    4   4.0   4.0   4.0    4   4.0   4.0   4.0     7     4     4   \n",
       "4   6.0   5.0    4   6.0   6.0   1.0    6   7.0   7.0   7.0     5     5     3   \n",
       "\n",
       "   q10d  q10e  q10f  q11a  q11b  q11c  q11d  q11e  q11f  q11g  q12a  q12b  \\\n",
       "0     3     6     6     4     4     6     3     6     3     2     6     2   \n",
       "1     6     6     6     6     6     6     6     6     6     6     4     2   \n",
       "2     7     7     7     6     6     7     6     6     6     6     3     1   \n",
       "3     4     4     4     4     4     6     4     5     4     4     7     4   \n",
       "4     4     4     4     5     4     6     5     5     5     6     6     4   \n",
       "\n",
       "   q12c  q12d  q131  q132  q133  q134  q135  q136  q137  q138  q139  q141  \\\n",
       "0     2     1     7     7     6     5     6     6     6     6     6     7   \n",
       "1     1     1     4     4     4     4     3     4     4     4     4     6   \n",
       "2     1     1     6     6     5     6     6     5     5     5     6     6   \n",
       "3     4     4     7     6     5     7     4     6     7     7     7     7   \n",
       "4     4     2     5     4     5     4     4     3     2     2     3     7   \n",
       "\n",
       "   q142  q143  q144  q145  q146  q151  q152  q153  q154  q155  q156  q157  \\\n",
       "0     5     2     6     5     6     6     3     6     4     5     3     7   \n",
       "1     6     3     4     6     5     4     3     6     6     5     5     6   \n",
       "2     6     2     2     7     7     2     4     7     7     7     4     4   \n",
       "3     7     5     5     4     4     7     4     4     1     6     1     4   \n",
       "4     7     1     1     7     7     1     7     7     6     7     3     1   \n",
       "\n",
       "   q158  q159  q1510  q16  q17  q18  q18b  q19  q20  q21_agev152  q22_sexv153  \\\n",
       "0     3     4      1  2.0  1.0  2.0   1.0  1.0  1.0            3            2   \n",
       "1     5     7      5  1.0  NaN  1.0   NaN  1.0  NaN            5            2   \n",
       "2     1     4      4  1.0  NaN  1.0   NaN  1.0  NaN            5            2   \n",
       "3     1     1      6  1.0  NaN  1.0   NaN  1.0  1.0            3            1   \n",
       "4     1     4      2  1.0  NaN  1.0   NaN  1.0  NaN            6            1   \n",
       "\n",
       "   q23_postcode  q24a  q24b  q24c  q24d  q24e  q24f  q25_incomev161  \n",
       "0        2010.0     0     0     1     0     0     0             4.0  \n",
       "1        2010.0     0     0     1     0     0     0             4.0  \n",
       "2        2011.0     0     0     1     0     0     0             6.0  \n",
       "3        2015.0     0     0     1     0     0     0             6.0  \n",
       "4        2016.0     0     0     0     0     1     0             5.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('Sports_Fans.csv', na_values = ' ')\n",
    "pd.set_option('display.max_columns', None)  # show all columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4717,
     "status": "ok",
     "timestamp": 1618474172519,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "0iExk4muEosu",
    "outputId": "798d52c0-d9d2-4656-89fb-d7bedf05b923",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: q11_id\n",
      "1: q12_age_int\n",
      "2: q13_age_attd\n",
      "3: q14_no_game\n",
      "4: q15_team_sup\n",
      "5: q16_attd_who\n",
      "6: mar28\n",
      "7: apr17\n",
      "8: may2\n",
      "9: may9\n",
      "10: may23\n",
      "11: june6\n",
      "12: june27\n",
      "13: july4\n",
      "14: july24\n",
      "15: july31\n",
      "16: aug15\n",
      "17: q18_game_last\n",
      "18: q19_sport_fx\n",
      "19: q21\n",
      "20: q22\n",
      "21: q23\n",
      "22: q24\n",
      "23: q25\n",
      "24: q26\n",
      "25: q27\n",
      "26: q28\n",
      "27: q29\n",
      "28: q210\n",
      "29: q211\n",
      "30: q3\n",
      "31: q41\n",
      "32: q42\n",
      "33: q43\n",
      "34: q44\n",
      "35: q45\n",
      "36: q46\n",
      "37: q51\n",
      "38: q52\n",
      "39: q53\n",
      "40: q54\n",
      "41: q55\n",
      "42: q56\n",
      "43: q57\n",
      "44: q58\n",
      "45: q61\n",
      "46: q62\n",
      "47: q63\n",
      "48: q64\n",
      "49: q65\n",
      "50: q71\n",
      "51: q711\n",
      "52: q712\n",
      "53: q72\n",
      "54: q721\n",
      "55: q722\n",
      "56: q723\n",
      "57: q73\n",
      "58: q731\n",
      "59: q732\n",
      "60: q74\n",
      "61: q741\n",
      "62: q742\n",
      "63: q75\n",
      "64: q751\n",
      "65: q81\n",
      "66: q82\n",
      "67: q83\n",
      "68: q84\n",
      "69: q85\n",
      "70: q91\n",
      "71: q911\n",
      "72: q912\n",
      "73: q92\n",
      "74: q921\n",
      "75: q922\n",
      "76: q923\n",
      "77: q924\n",
      "78: q93\n",
      "79: q931\n",
      "80: q932\n",
      "81: q933\n",
      "82: q934\n",
      "83: q94\n",
      "84: q941\n",
      "85: q942\n",
      "86: q943\n",
      "87: q95\n",
      "88: q951\n",
      "89: q952\n",
      "90: q953\n",
      "91: q10a\n",
      "92: q10b\n",
      "93: q10c\n",
      "94: q10d\n",
      "95: q10e\n",
      "96: q10f\n",
      "97: q11a\n",
      "98: q11b\n",
      "99: q11c\n",
      "100: q11d\n",
      "101: q11e\n",
      "102: q11f\n",
      "103: q11g\n",
      "104: q12a\n",
      "105: q12b\n",
      "106: q12c\n",
      "107: q12d\n",
      "108: q131\n",
      "109: q132\n",
      "110: q133\n",
      "111: q134\n",
      "112: q135\n",
      "113: q136\n",
      "114: q137\n",
      "115: q138\n",
      "116: q139\n",
      "117: q141\n",
      "118: q142\n",
      "119: q143\n",
      "120: q144\n",
      "121: q145\n",
      "122: q146\n",
      "123: q151\n",
      "124: q152\n",
      "125: q153\n",
      "126: q154\n",
      "127: q155\n",
      "128: q156\n",
      "129: q157\n",
      "130: q158\n",
      "131: q159\n",
      "132: q1510\n",
      "133: q16\n",
      "134: q17\n",
      "135: q18\n",
      "136: q18b\n",
      "137: q19\n",
      "138: q20\n",
      "139: q21_agev152\n",
      "140: q22_sexv153\n",
      "141: q23_postcode\n",
      "142: q24a\n",
      "143: q24b\n",
      "144: q24c\n",
      "145: q24d\n",
      "146: q24e\n",
      "147: q24f\n",
      "148: q25_incomev161\n"
     ]
    }
   ],
   "source": [
    "column_name = df.columns\n",
    "i = 0\n",
    "\n",
    "for n in column_name:\n",
    "    print(f'{str(i)}: {n}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "executionInfo": {
     "elapsed": 4707,
     "status": "ok",
     "timestamp": 1618474172520,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "OmdbYZNFEosv",
    "outputId": "b08188e7-8370-4073-a6f8-481534981490"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q11_id</th>\n",
       "      <th>q12_age_int</th>\n",
       "      <th>q13_age_attd</th>\n",
       "      <th>q14_no_game</th>\n",
       "      <th>q15_team_sup</th>\n",
       "      <th>q16_attd_who</th>\n",
       "      <th>mar28</th>\n",
       "      <th>apr17</th>\n",
       "      <th>may2</th>\n",
       "      <th>may9</th>\n",
       "      <th>may23</th>\n",
       "      <th>june6</th>\n",
       "      <th>june27</th>\n",
       "      <th>july4</th>\n",
       "      <th>july24</th>\n",
       "      <th>july31</th>\n",
       "      <th>aug15</th>\n",
       "      <th>q18_game_last</th>\n",
       "      <th>q19_sport_fx</th>\n",
       "      <th>q21</th>\n",
       "      <th>q22</th>\n",
       "      <th>q23</th>\n",
       "      <th>q24</th>\n",
       "      <th>q25</th>\n",
       "      <th>q26</th>\n",
       "      <th>q27</th>\n",
       "      <th>q28</th>\n",
       "      <th>q29</th>\n",
       "      <th>q210</th>\n",
       "      <th>q211</th>\n",
       "      <th>q3</th>\n",
       "      <th>q41</th>\n",
       "      <th>q42</th>\n",
       "      <th>q43</th>\n",
       "      <th>q44</th>\n",
       "      <th>q45</th>\n",
       "      <th>q46</th>\n",
       "      <th>q51</th>\n",
       "      <th>q52</th>\n",
       "      <th>q53</th>\n",
       "      <th>q54</th>\n",
       "      <th>q55</th>\n",
       "      <th>q56</th>\n",
       "      <th>q57</th>\n",
       "      <th>q58</th>\n",
       "      <th>q61</th>\n",
       "      <th>q62</th>\n",
       "      <th>q63</th>\n",
       "      <th>q64</th>\n",
       "      <th>q65</th>\n",
       "      <th>q71</th>\n",
       "      <th>q711</th>\n",
       "      <th>q712</th>\n",
       "      <th>q72</th>\n",
       "      <th>q721</th>\n",
       "      <th>q722</th>\n",
       "      <th>q723</th>\n",
       "      <th>q73</th>\n",
       "      <th>q731</th>\n",
       "      <th>q732</th>\n",
       "      <th>q74</th>\n",
       "      <th>q741</th>\n",
       "      <th>q742</th>\n",
       "      <th>q75</th>\n",
       "      <th>q751</th>\n",
       "      <th>q81</th>\n",
       "      <th>q82</th>\n",
       "      <th>q83</th>\n",
       "      <th>q84</th>\n",
       "      <th>q85</th>\n",
       "      <th>q91</th>\n",
       "      <th>q911</th>\n",
       "      <th>q912</th>\n",
       "      <th>q92</th>\n",
       "      <th>q921</th>\n",
       "      <th>q922</th>\n",
       "      <th>q923</th>\n",
       "      <th>q924</th>\n",
       "      <th>q93</th>\n",
       "      <th>q931</th>\n",
       "      <th>q932</th>\n",
       "      <th>q933</th>\n",
       "      <th>q934</th>\n",
       "      <th>q94</th>\n",
       "      <th>q941</th>\n",
       "      <th>q942</th>\n",
       "      <th>q943</th>\n",
       "      <th>q95</th>\n",
       "      <th>q951</th>\n",
       "      <th>q952</th>\n",
       "      <th>q953</th>\n",
       "      <th>q10a</th>\n",
       "      <th>q10b</th>\n",
       "      <th>q10c</th>\n",
       "      <th>q10d</th>\n",
       "      <th>q10e</th>\n",
       "      <th>q10f</th>\n",
       "      <th>q11a</th>\n",
       "      <th>q11b</th>\n",
       "      <th>q11c</th>\n",
       "      <th>q11d</th>\n",
       "      <th>q11e</th>\n",
       "      <th>q11f</th>\n",
       "      <th>q11g</th>\n",
       "      <th>q12a</th>\n",
       "      <th>q12b</th>\n",
       "      <th>q12c</th>\n",
       "      <th>q12d</th>\n",
       "      <th>q131</th>\n",
       "      <th>q132</th>\n",
       "      <th>q133</th>\n",
       "      <th>q134</th>\n",
       "      <th>q135</th>\n",
       "      <th>q136</th>\n",
       "      <th>q137</th>\n",
       "      <th>q138</th>\n",
       "      <th>q139</th>\n",
       "      <th>q141</th>\n",
       "      <th>q142</th>\n",
       "      <th>q143</th>\n",
       "      <th>q144</th>\n",
       "      <th>q145</th>\n",
       "      <th>q146</th>\n",
       "      <th>q151</th>\n",
       "      <th>q152</th>\n",
       "      <th>q153</th>\n",
       "      <th>q154</th>\n",
       "      <th>q155</th>\n",
       "      <th>q156</th>\n",
       "      <th>q157</th>\n",
       "      <th>q158</th>\n",
       "      <th>q159</th>\n",
       "      <th>q1510</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>968</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>733</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>389</td>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>924</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>762</td>\n",
       "      <td>37.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   q11_id  q12_age_int  q13_age_attd  q14_no_game  q15_team_sup  q16_attd_who  \\\n",
       "0     968          5.0          12.0          2.0           2.0           3.0   \n",
       "1     733         20.0          20.0          3.0           1.0           3.0   \n",
       "2     389         14.0          20.0          4.0           2.0           3.0   \n",
       "3     924         30.0          30.0          3.0           1.0           3.0   \n",
       "4     762         37.0          34.0          3.0           1.0           2.0   \n",
       "\n",
       "   mar28  apr17  may2  may9  may23  june6  june27  july4  july24  july31  \\\n",
       "0      1      0     1     1      0      0       0      1       1       1   \n",
       "1      0      0     0     1      1      1       1      1       1       1   \n",
       "2      1      1     1     1      1      1       1      1       1       1   \n",
       "3      1      1     1     1      1      0       0      0       0       0   \n",
       "4      1      1     1     1      1      1       1      1       1       1   \n",
       "\n",
       "   aug15  q18_game_last  q19_sport_fx  q21  q22  q23  q24  q25  q26  q27  q28  \\\n",
       "0      1            1.0           3.0    6    1    7  7.0  1.0  7.0    2    5   \n",
       "1      1            1.0           4.0    1    1    6  7.0  4.0  6.0    4    5   \n",
       "2      1            2.0           1.0    3    1    5  5.0  5.0  4.0    5    5   \n",
       "3      0            1.0           4.0    1    1    7  7.0  1.0  4.0    1    7   \n",
       "4      1            2.0           3.0    7    4    4  7.0  7.0  2.0    6    7   \n",
       "\n",
       "   q29  q210  q211   q3  q41  q42  q43  q44  q45  q46  q51  q52  q53  q54  \\\n",
       "0  7.0   2.0   2.0  5.0    2    2    5    6    2    1    2    4    5    5   \n",
       "1  4.0   1.0   7.0  4.0    5    4    7    7    6    5    5    5    7    7   \n",
       "2  5.0   4.0   4.0  7.0    4    3    4    6    6    5    7    5    7    7   \n",
       "3  7.0   1.0   4.0  4.0    1    1    1    2    1    1    1    1    4    4   \n",
       "4  7.0   2.0   7.0  7.0    7    4    7    6    7    4    7    7    7    7   \n",
       "\n",
       "   q55  q56  q57  q58  q61   q62   q63   q64  q65  q71  q711  q712  q72  q721  \\\n",
       "0    1    1    1    1   30  40.0  20.0  10.0    0    6   5.0   7.0    6   4.0   \n",
       "1    5    5    4    6   30  20.0  20.0  20.0   10    4   4.0   4.0    4   4.0   \n",
       "2    5    5    4    4   20  25.0  25.0  20.0   10    6   6.0   6.0    5   5.0   \n",
       "3    1    1    1    1   20  20.0  30.0  20.0   10    4   4.0   4.0    4   4.0   \n",
       "4    4    6    1    5   10  20.0  40.0  25.0    5    5   5.0   2.0    4   2.0   \n",
       "\n",
       "   q722  q723  q73  q731  q732  q74  q741  q742  q75  q751   q81   q82   q83  \\\n",
       "0   6.0   6.0    6   7.0   6.0    3   4.0   5.0    6   4.0  10.0  15.0  15.0   \n",
       "1   5.0   4.0    4   4.0   4.0    5   5.0   5.0    5   4.0  20.0  20.0  20.0   \n",
       "2   6.0   4.0    5   5.0   5.0    5   7.0   7.0    6   5.0  20.0  25.0  20.0   \n",
       "3   4.0   4.0    4   4.0   4.0    4   4.0   4.0    4   4.0  30.0  10.0  10.0   \n",
       "4   3.0   3.0    4   3.0   2.0    3   3.0   4.0    2   5.0  25.0   5.0  20.0   \n",
       "\n",
       "    q84   q85  q91  q911  q912  q92  q921  q922  q923  q924  q93  q931  q932  \\\n",
       "0  30.0  30.0    6   6.0   5.0    6   6.0   7.0   5.0   6.0    6   5.0   7.0   \n",
       "1  20.0  20.0    4   5.0   6.0    4   5.0   5.0   5.0   6.0    5   5.0   5.0   \n",
       "2  20.0  15.0    5   6.0   4.0    5   7.0   5.0   5.0   5.0    5   5.0   6.0   \n",
       "3  20.0  30.0    4   4.0   4.0    4   4.0   4.0   4.0   4.0    4   4.0   4.0   \n",
       "4  40.0  10.0    6   6.0   6.0    4   2.0   1.0   1.0   3.0    3   2.0   5.0   \n",
       "\n",
       "   q933  q934  q94  q941  q942  q943  q95  q951  q952  q953  q10a  q10b  q10c  \\\n",
       "0   6.0   5.0    6   6.0   6.0   6.0    7   7.0   7.0   7.0     2     3     5   \n",
       "1   4.0   5.0    5   6.0   6.0   6.0    5   6.0   6.0   6.0     5     6     6   \n",
       "2   4.0   5.0    6   6.0   7.0   5.0    5   7.0   5.0   6.0     7     7     7   \n",
       "3   4.0   4.0    4   4.0   4.0   4.0    4   4.0   4.0   4.0     7     4     4   \n",
       "4   6.0   5.0    4   6.0   6.0   1.0    6   7.0   7.0   7.0     5     5     3   \n",
       "\n",
       "   q10d  q10e  q10f  q11a  q11b  q11c  q11d  q11e  q11f  q11g  q12a  q12b  \\\n",
       "0     3     6     6     4     4     6     3     6     3     2     6     2   \n",
       "1     6     6     6     6     6     6     6     6     6     6     4     2   \n",
       "2     7     7     7     6     6     7     6     6     6     6     3     1   \n",
       "3     4     4     4     4     4     6     4     5     4     4     7     4   \n",
       "4     4     4     4     5     4     6     5     5     5     6     6     4   \n",
       "\n",
       "   q12c  q12d  q131  q132  q133  q134  q135  q136  q137  q138  q139  q141  \\\n",
       "0     2     1     7     7     6     5     6     6     6     6     6     7   \n",
       "1     1     1     4     4     4     4     3     4     4     4     4     6   \n",
       "2     1     1     6     6     5     6     6     5     5     5     6     6   \n",
       "3     4     4     7     6     5     7     4     6     7     7     7     7   \n",
       "4     4     2     5     4     5     4     4     3     2     2     3     7   \n",
       "\n",
       "   q142  q143  q144  q145  q146  q151  q152  q153  q154  q155  q156  q157  \\\n",
       "0     5     2     6     5     6     6     3     6     4     5     3     7   \n",
       "1     6     3     4     6     5     4     3     6     6     5     5     6   \n",
       "2     6     2     2     7     7     2     4     7     7     7     4     4   \n",
       "3     7     5     5     4     4     7     4     4     1     6     1     4   \n",
       "4     7     1     1     7     7     1     7     7     6     7     3     1   \n",
       "\n",
       "   q158  q159  q1510  \n",
       "0     3     4      1  \n",
       "1     5     7      5  \n",
       "2     1     4      4  \n",
       "3     1     1      6  \n",
       "4     1     4      2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the background questions columns\n",
    "# columns with index 0-132\n",
    "column_name = column_name[0:133]\n",
    "df_dropped = df[column_name]\n",
    "df_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "c_a1QTDbEosw"
   },
   "outputs": [],
   "source": [
    "# select columns by its position\n",
    "attend_sum = range(6, 17) # columns with index 6-16\n",
    "attend_sum = column_name[attend_sum]\n",
    "\n",
    "proportion_6 = range(45, 50)\n",
    "proportion_6 = column_name[proportion_6]\n",
    "df_proportion_6 = df[proportion_6].fillna(0)[proportion_6]\n",
    "\n",
    "scale_7 = range(50, 65)\n",
    "scale_7 = column_name[scale_7]\n",
    "df_scale_7 = df[scale_7].fillna(df[scale_7].mean())[scale_7]\n",
    "\n",
    "proportion_8 = range(65, 70)\n",
    "proportion_8 = column_name[proportion_8]\n",
    "df_proportion_8 = df[proportion_8].fillna(0)[proportion_8]\n",
    "\n",
    "scale_9 = range(70, 91)\n",
    "scale_9 = column_name[scale_9]\n",
    "df_scale_9 = df[scale_9].fillna(df[scale_9].mean())[scale_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the remain columns used to do average\n",
    "ls_avg = []\n",
    "drop_col = list(attend_sum) + list(proportion_6) + list(scale_7) + list(proportion_8) + list(scale_9)\n",
    "\n",
    "for x in column_name:\n",
    "    if x not in drop_col:\n",
    "        ls_avg.append(x)\n",
    "\n",
    "df_avg = df[ls_avg].fillna(df[ls_avg].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mIVEFp1JEosy"
   },
   "outputs": [],
   "source": [
    "# deal with attend_sum\n",
    "df_attend_sum = df[attend_sum]\n",
    "attend_sum_result = df_attend_sum.sum(axis=1) # dtype = pandas.series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PsIYVtbrEosz"
   },
   "outputs": [],
   "source": [
    "# a function to make proportion sum up to 100\n",
    "def standard_100(ls):\n",
    "    sum_up = sum(ls)\n",
    "    if sum_up != 100 and sum_up != 0:\n",
    "        n = sum_up/100\n",
    "        ls = [x / n for x in ls]\n",
    "    else:\n",
    "        pass\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YLZZWPm7Eos0"
   },
   "outputs": [],
   "source": [
    "# deal with proportion 6 and proportion 8\n",
    "df_proportion_6_to100 = pd.DataFrame(columns=df_proportion_6.columns)\n",
    "\n",
    "for i in range(len(df['q11_id'])):\n",
    "    res = standard_100(list(df_proportion_6.iloc[i,:]))\n",
    "    df_proportion_6_to100.loc[i] = res\n",
    "\n",
    "    \n",
    "df_proportion_8_to100 = pd.DataFrame(columns=df_proportion_8.columns)\n",
    "\n",
    "for i in range(len(df['q11_id'])):\n",
    "    res = standard_100(list(df_proportion_8.iloc[i,:]))\n",
    "    df_proportion_8_to100.loc[i] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "executionInfo": {
     "elapsed": 8075,
     "status": "ok",
     "timestamp": 1618474175907,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "RuieyKGlEos0",
    "outputId": "8899921b-5c8c-4c9c-f2bc-970a30d8b35b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q61</th>\n",
       "      <th>q62</th>\n",
       "      <th>q63</th>\n",
       "      <th>q64</th>\n",
       "      <th>q65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>44.444444</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>11.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>674 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           q61        q62        q63        q64        q65\n",
       "0    30.000000  40.000000  20.000000  10.000000   0.000000\n",
       "1    30.000000  20.000000  20.000000  20.000000  10.000000\n",
       "2    20.000000  25.000000  25.000000  20.000000  10.000000\n",
       "3    20.000000  20.000000  30.000000  20.000000  10.000000\n",
       "4    10.000000  20.000000  40.000000  25.000000   5.000000\n",
       "..         ...        ...        ...        ...        ...\n",
       "669  44.444444  16.666667  16.666667  11.111111  11.111111\n",
       "670  50.000000  20.000000  10.000000  10.000000  10.000000\n",
       "671  20.000000  30.000000  20.000000  30.000000   0.000000\n",
       "672  10.000000  50.000000  20.000000  15.000000   5.000000\n",
       "673  20.000000  20.000000  20.000000  30.000000  10.000000\n",
       "\n",
       "[674 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_proportion_6_to100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zkz47p6NEos1"
   },
   "outputs": [],
   "source": [
    "# make the weight matrix function based on previous proportion 6 and 8\n",
    "def proportion_7_matrix(ls):\n",
    "    seven_1 = [ls[0]]*3\n",
    "    seven_2 = [ls[1]]*4\n",
    "    seven_3 = [ls[2]]*3\n",
    "    seven_4 = [ls[3]]*3\n",
    "    seven_5 = [ls[4]]*2\n",
    "    ls = seven_1 + seven_2 + seven_3 + seven_4 + seven_5\n",
    "    return ls\n",
    "\n",
    "def proportion_9_matrix(ls):\n",
    "    seven_1 = [ls[0]]*3\n",
    "    seven_2 = [ls[1]]*5\n",
    "    seven_3 = [ls[2]]*5\n",
    "    seven_4 = [ls[3]]*4\n",
    "    seven_5 = [ls[4]]*4\n",
    "    ls = seven_1 + seven_2 + seven_3 + seven_4 + seven_5\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ga0qCfE-Eos2"
   },
   "outputs": [],
   "source": [
    "# make the weight matrix by function\n",
    "proportion_scale_7 = pd.DataFrame(columns=df_scale_7.columns)\n",
    "\n",
    "for i in range(len(df['q11_id'])):\n",
    "    res = proportion_7_matrix(list(df_proportion_6_to100.iloc[i,:]))\n",
    "    proportion_scale_7.loc[i] = res\n",
    "\n",
    "    \n",
    "proportion_scale_9 = pd.DataFrame(columns=df_scale_9.columns)\n",
    "\n",
    "for i in range(len(df['q11_id'])):\n",
    "    res = proportion_9_matrix(list(df_proportion_8_to100.iloc[i,:]))\n",
    "    proportion_scale_9.loc[i] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "executionInfo": {
     "elapsed": 10745,
     "status": "ok",
     "timestamp": 1618474178591,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "lT_L25TsEos3",
    "outputId": "7372e20f-a2d6-44ed-bbbc-3d430205fb8f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q71</th>\n",
       "      <th>q711</th>\n",
       "      <th>q712</th>\n",
       "      <th>q72</th>\n",
       "      <th>q721</th>\n",
       "      <th>q722</th>\n",
       "      <th>q723</th>\n",
       "      <th>q73</th>\n",
       "      <th>q731</th>\n",
       "      <th>q732</th>\n",
       "      <th>q74</th>\n",
       "      <th>q741</th>\n",
       "      <th>q742</th>\n",
       "      <th>q75</th>\n",
       "      <th>q751</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>674 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     q71  q711  q712  q72  q721  q722  q723  q73  q731  q732  q74  q741  q742  \\\n",
       "0      6   5.0   7.0    6   4.0   6.0   6.0    6   7.0   6.0    3   4.0   5.0   \n",
       "1      4   4.0   4.0    4   4.0   5.0   4.0    4   4.0   4.0    5   5.0   5.0   \n",
       "2      6   6.0   6.0    5   5.0   6.0   4.0    5   5.0   5.0    5   7.0   7.0   \n",
       "3      4   4.0   4.0    4   4.0   4.0   4.0    4   4.0   4.0    4   4.0   4.0   \n",
       "4      5   5.0   2.0    4   2.0   3.0   3.0    4   3.0   2.0    3   3.0   4.0   \n",
       "..   ...   ...   ...  ...   ...   ...   ...  ...   ...   ...  ...   ...   ...   \n",
       "669    6   4.0   6.0    6   6.0   5.0   5.0    6   5.0   5.0    6   5.0   7.0   \n",
       "670    4   5.0   3.0    5   5.0   5.0   6.0    6   5.0   6.0    5   5.0   6.0   \n",
       "671    5   4.0   5.0    4   4.0   5.0   5.0    6   5.0   6.0    6   6.0   6.0   \n",
       "672    5   5.0   6.0    4   5.0   3.0   3.0    6   6.0   6.0    6   6.0   6.0   \n",
       "673    5   5.0   6.0    6   5.0   6.0   5.0    6   6.0   5.0    6   5.0   5.0   \n",
       "\n",
       "     q75  q751  \n",
       "0      6   4.0  \n",
       "1      5   4.0  \n",
       "2      6   5.0  \n",
       "3      4   4.0  \n",
       "4      2   5.0  \n",
       "..   ...   ...  \n",
       "669    7   6.0  \n",
       "670    5   4.0  \n",
       "671    6   4.0  \n",
       "672    5   5.0  \n",
       "673    6   5.0  \n",
       "\n",
       "[674 rows x 15 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scale_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "executionInfo": {
     "elapsed": 10736,
     "status": "ok",
     "timestamp": 1618474178592,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "HQCQTXW8Eos4",
    "outputId": "f9d773be-2a39-4179-df95-ce21f65685e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q71</th>\n",
       "      <th>q711</th>\n",
       "      <th>q712</th>\n",
       "      <th>q72</th>\n",
       "      <th>q721</th>\n",
       "      <th>q722</th>\n",
       "      <th>q723</th>\n",
       "      <th>q73</th>\n",
       "      <th>q731</th>\n",
       "      <th>q732</th>\n",
       "      <th>q74</th>\n",
       "      <th>q741</th>\n",
       "      <th>q742</th>\n",
       "      <th>q75</th>\n",
       "      <th>q751</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>11.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>674 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           q71       q711       q712        q72       q721       q722  \\\n",
       "0    30.000000  30.000000  30.000000  40.000000  40.000000  40.000000   \n",
       "1    30.000000  30.000000  30.000000  20.000000  20.000000  20.000000   \n",
       "2    20.000000  20.000000  20.000000  25.000000  25.000000  25.000000   \n",
       "3    20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
       "4    10.000000  10.000000  10.000000  20.000000  20.000000  20.000000   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "669  44.444444  44.444444  44.444444  16.666667  16.666667  16.666667   \n",
       "670  50.000000  50.000000  50.000000  20.000000  20.000000  20.000000   \n",
       "671  20.000000  20.000000  20.000000  30.000000  30.000000  30.000000   \n",
       "672  10.000000  10.000000  10.000000  50.000000  50.000000  50.000000   \n",
       "673  20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
       "\n",
       "          q723        q73       q731       q732        q74       q741  \\\n",
       "0    40.000000  20.000000  20.000000  20.000000  10.000000  10.000000   \n",
       "1    20.000000  20.000000  20.000000  20.000000  20.000000  20.000000   \n",
       "2    25.000000  25.000000  25.000000  25.000000  20.000000  20.000000   \n",
       "3    20.000000  30.000000  30.000000  30.000000  20.000000  20.000000   \n",
       "4    20.000000  40.000000  40.000000  40.000000  25.000000  25.000000   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "669  16.666667  16.666667  16.666667  16.666667  11.111111  11.111111   \n",
       "670  20.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \n",
       "671  30.000000  20.000000  20.000000  20.000000  30.000000  30.000000   \n",
       "672  50.000000  20.000000  20.000000  20.000000  15.000000  15.000000   \n",
       "673  20.000000  20.000000  20.000000  20.000000  30.000000  30.000000   \n",
       "\n",
       "          q742        q75       q751  \n",
       "0    10.000000   0.000000   0.000000  \n",
       "1    20.000000  10.000000  10.000000  \n",
       "2    20.000000  10.000000  10.000000  \n",
       "3    20.000000  10.000000  10.000000  \n",
       "4    25.000000   5.000000   5.000000  \n",
       "..         ...        ...        ...  \n",
       "669  11.111111  11.111111  11.111111  \n",
       "670  10.000000  10.000000  10.000000  \n",
       "671  30.000000   0.000000   0.000000  \n",
       "672  15.000000   5.000000   5.000000  \n",
       "673  30.000000  10.000000  10.000000  \n",
       "\n",
       "[674 rows x 15 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proportion_scale_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4xr-f9PCEos5"
   },
   "outputs": [],
   "source": [
    "# make the product between weight and scale of each question\n",
    "product_scale_7 = pd.DataFrame(columns=df_scale_7.columns)\n",
    "\n",
    "for i in range(len(df['q11_id'])):\n",
    "    res = np.array(df_scale_7.iloc[i,:]) * (np.array(proportion_scale_7.iloc[i,:])/100)\n",
    "    product_scale_7.loc[i] = res\n",
    "\n",
    "    \n",
    "product_scale_9 = pd.DataFrame(columns=df_scale_9.columns)\n",
    "\n",
    "for i in range(len(df['q11_id'])):\n",
    "    res = np.array(df_scale_9.iloc[i,:]) * (np.array(proportion_scale_9.iloc[i,:])/100)\n",
    "    product_scale_9.loc[i] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "executionInfo": {
     "elapsed": 13696,
     "status": "ok",
     "timestamp": 1618474181563,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "Ag4ECxmqEos6",
    "outputId": "808cb327-e043-44a7-8edd-d8983720a20a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q71</th>\n",
       "      <th>q711</th>\n",
       "      <th>q712</th>\n",
       "      <th>q72</th>\n",
       "      <th>q721</th>\n",
       "      <th>q722</th>\n",
       "      <th>q723</th>\n",
       "      <th>q73</th>\n",
       "      <th>q731</th>\n",
       "      <th>q732</th>\n",
       "      <th>q74</th>\n",
       "      <th>q741</th>\n",
       "      <th>q742</th>\n",
       "      <th>q75</th>\n",
       "      <th>q751</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>674 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          q71      q711      q712   q72  q721      q722      q723   q73  \\\n",
       "0    1.800000  1.500000  2.100000  2.40  1.60  2.400000  2.400000  1.20   \n",
       "1    1.200000  1.200000  1.200000  0.80  0.80  1.000000  0.800000  0.80   \n",
       "2    1.200000  1.200000  1.200000  1.25  1.25  1.500000  1.000000  1.25   \n",
       "3    0.800000  0.800000  0.800000  0.80  0.80  0.800000  0.800000  1.20   \n",
       "4    0.500000  0.500000  0.200000  0.80  0.40  0.600000  0.600000  1.60   \n",
       "..        ...       ...       ...   ...   ...       ...       ...   ...   \n",
       "669  2.666667  1.777778  2.666667  1.00  1.00  0.833333  0.833333  1.00   \n",
       "670  2.000000  2.500000  1.500000  1.00  1.00  1.000000  1.200000  0.60   \n",
       "671  1.000000  0.800000  1.000000  1.20  1.20  1.500000  1.500000  1.20   \n",
       "672  0.500000  0.500000  0.600000  2.00  2.50  1.500000  1.500000  1.20   \n",
       "673  1.000000  1.000000  1.200000  1.20  1.00  1.200000  1.000000  1.20   \n",
       "\n",
       "         q731      q732       q74      q741      q742       q75      q751  \n",
       "0    1.400000  1.200000  0.300000  0.400000  0.500000  0.000000  0.000000  \n",
       "1    0.800000  0.800000  1.000000  1.000000  1.000000  0.500000  0.400000  \n",
       "2    1.250000  1.250000  1.000000  1.400000  1.400000  0.600000  0.500000  \n",
       "3    1.200000  1.200000  0.800000  0.800000  0.800000  0.400000  0.400000  \n",
       "4    1.200000  0.800000  0.750000  0.750000  1.000000  0.100000  0.250000  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "669  0.833333  0.833333  0.666667  0.555556  0.777778  0.777778  0.666667  \n",
       "670  0.500000  0.600000  0.500000  0.500000  0.600000  0.500000  0.400000  \n",
       "671  1.000000  1.200000  1.800000  1.800000  1.800000  0.000000  0.000000  \n",
       "672  1.200000  1.200000  0.900000  0.900000  0.900000  0.250000  0.250000  \n",
       "673  1.200000  1.000000  1.800000  1.500000  1.500000  0.600000  0.500000  \n",
       "\n",
       "[674 rows x 15 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_scale_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13683,
     "status": "ok",
     "timestamp": 1618474181564,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "diVrxXMYq_O3",
    "outputId": "5493cbfa-cd5a-47d5-d294-b3d04701075a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q21_agev152        0\n",
      "q22_sexv153        0\n",
      "q25_incomev161    10\n",
      "dtype: int64\n",
      "q21_agev152       0\n",
      "q22_sexv153       0\n",
      "q25_incomev161    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# handle the value null for demographics\n",
    "demograph = ['q21_agev152', 'q22_sexv153', 'q25_incomev161']\n",
    "\n",
    "df_demograph = df[demograph]\n",
    "print(df_demograph.isna().sum())\n",
    "\n",
    "df_demograph = df[demograph].fillna(df[demograph].mean())\n",
    "print(df_demograph.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "executionInfo": {
     "elapsed": 13664,
     "status": "ok",
     "timestamp": 1618474181565,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "fjjjPO61rs5l",
    "outputId": "3b363956-f8a6-49d2-c940-38f32425eb73"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q12_age_int</th>\n",
       "      <th>q13_age_attd</th>\n",
       "      <th>q14_no_game</th>\n",
       "      <th>q15_team_sup</th>\n",
       "      <th>q16_attd_who</th>\n",
       "      <th>q18_game_last</th>\n",
       "      <th>q19_sport_fx</th>\n",
       "      <th>q21</th>\n",
       "      <th>q22</th>\n",
       "      <th>q23</th>\n",
       "      <th>q24</th>\n",
       "      <th>q25</th>\n",
       "      <th>q26</th>\n",
       "      <th>q27</th>\n",
       "      <th>q28</th>\n",
       "      <th>q29</th>\n",
       "      <th>q210</th>\n",
       "      <th>q211</th>\n",
       "      <th>q3</th>\n",
       "      <th>q41</th>\n",
       "      <th>q42</th>\n",
       "      <th>q43</th>\n",
       "      <th>q44</th>\n",
       "      <th>q45</th>\n",
       "      <th>q46</th>\n",
       "      <th>q51</th>\n",
       "      <th>q52</th>\n",
       "      <th>q53</th>\n",
       "      <th>q54</th>\n",
       "      <th>q55</th>\n",
       "      <th>q56</th>\n",
       "      <th>q57</th>\n",
       "      <th>q58</th>\n",
       "      <th>q10a</th>\n",
       "      <th>q10b</th>\n",
       "      <th>q10c</th>\n",
       "      <th>q10d</th>\n",
       "      <th>q10e</th>\n",
       "      <th>q10f</th>\n",
       "      <th>q11a</th>\n",
       "      <th>q11b</th>\n",
       "      <th>q11c</th>\n",
       "      <th>q11d</th>\n",
       "      <th>q11e</th>\n",
       "      <th>q11f</th>\n",
       "      <th>q11g</th>\n",
       "      <th>q12a</th>\n",
       "      <th>q12b</th>\n",
       "      <th>q12c</th>\n",
       "      <th>q12d</th>\n",
       "      <th>q131</th>\n",
       "      <th>q132</th>\n",
       "      <th>q133</th>\n",
       "      <th>q134</th>\n",
       "      <th>q135</th>\n",
       "      <th>q136</th>\n",
       "      <th>q137</th>\n",
       "      <th>q138</th>\n",
       "      <th>q139</th>\n",
       "      <th>q141</th>\n",
       "      <th>q142</th>\n",
       "      <th>q143</th>\n",
       "      <th>q144</th>\n",
       "      <th>q145</th>\n",
       "      <th>q146</th>\n",
       "      <th>q151</th>\n",
       "      <th>q152</th>\n",
       "      <th>q153</th>\n",
       "      <th>q154</th>\n",
       "      <th>q155</th>\n",
       "      <th>q156</th>\n",
       "      <th>q157</th>\n",
       "      <th>q158</th>\n",
       "      <th>q159</th>\n",
       "      <th>q1510</th>\n",
       "      <th>q71</th>\n",
       "      <th>q711</th>\n",
       "      <th>q712</th>\n",
       "      <th>q72</th>\n",
       "      <th>q721</th>\n",
       "      <th>q722</th>\n",
       "      <th>q723</th>\n",
       "      <th>q73</th>\n",
       "      <th>q731</th>\n",
       "      <th>q732</th>\n",
       "      <th>q74</th>\n",
       "      <th>q741</th>\n",
       "      <th>q742</th>\n",
       "      <th>q75</th>\n",
       "      <th>q751</th>\n",
       "      <th>q91</th>\n",
       "      <th>q911</th>\n",
       "      <th>q912</th>\n",
       "      <th>q92</th>\n",
       "      <th>q921</th>\n",
       "      <th>q922</th>\n",
       "      <th>q923</th>\n",
       "      <th>q924</th>\n",
       "      <th>q93</th>\n",
       "      <th>q931</th>\n",
       "      <th>q932</th>\n",
       "      <th>q933</th>\n",
       "      <th>q934</th>\n",
       "      <th>q94</th>\n",
       "      <th>q941</th>\n",
       "      <th>q942</th>\n",
       "      <th>q943</th>\n",
       "      <th>q95</th>\n",
       "      <th>q951</th>\n",
       "      <th>q952</th>\n",
       "      <th>q953</th>\n",
       "      <th>attendance_count</th>\n",
       "      <th>q21_agev152</th>\n",
       "      <th>q22_sexv153</th>\n",
       "      <th>q25_incomev161</th>\n",
       "      <th>stay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.9</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   q12_age_int  q13_age_attd  q14_no_game  q15_team_sup  q16_attd_who  \\\n",
       "0          5.0          12.0          2.0           2.0           3.0   \n",
       "1         20.0          20.0          3.0           1.0           3.0   \n",
       "2         14.0          20.0          4.0           2.0           3.0   \n",
       "3         30.0          30.0          3.0           1.0           3.0   \n",
       "4         37.0          34.0          3.0           1.0           2.0   \n",
       "\n",
       "   q18_game_last  q19_sport_fx  q21  q22  q23  q24  q25  q26  q27  q28  q29  \\\n",
       "0            1.0           3.0    6    1    7  7.0  1.0  7.0    2    5  7.0   \n",
       "1            1.0           4.0    1    1    6  7.0  4.0  6.0    4    5  4.0   \n",
       "2            2.0           1.0    3    1    5  5.0  5.0  4.0    5    5  5.0   \n",
       "3            1.0           4.0    1    1    7  7.0  1.0  4.0    1    7  7.0   \n",
       "4            2.0           3.0    7    4    4  7.0  7.0  2.0    6    7  7.0   \n",
       "\n",
       "   q210  q211   q3  q41  q42  q43  q44  q45  q46  q51  q52  q53  q54  q55  \\\n",
       "0   2.0   2.0  5.0    2    2    5    6    2    1    2    4    5    5    1   \n",
       "1   1.0   7.0  4.0    5    4    7    7    6    5    5    5    7    7    5   \n",
       "2   4.0   4.0  7.0    4    3    4    6    6    5    7    5    7    7    5   \n",
       "3   1.0   4.0  4.0    1    1    1    2    1    1    1    1    4    4    1   \n",
       "4   2.0   7.0  7.0    7    4    7    6    7    4    7    7    7    7    4   \n",
       "\n",
       "   q56  q57  q58  q10a  q10b  q10c  q10d  q10e  q10f  q11a  q11b  q11c  q11d  \\\n",
       "0    1    1    1     2     3     5     3     6     6     4     4     6     3   \n",
       "1    5    4    6     5     6     6     6     6     6     6     6     6     6   \n",
       "2    5    4    4     7     7     7     7     7     7     6     6     7     6   \n",
       "3    1    1    1     7     4     4     4     4     4     4     4     6     4   \n",
       "4    6    1    5     5     5     3     4     4     4     5     4     6     5   \n",
       "\n",
       "   q11e  q11f  q11g  q12a  q12b  q12c  q12d  q131  q132  q133  q134  q135  \\\n",
       "0     6     3     2     6     2     2     1     7     7     6     5     6   \n",
       "1     6     6     6     4     2     1     1     4     4     4     4     3   \n",
       "2     6     6     6     3     1     1     1     6     6     5     6     6   \n",
       "3     5     4     4     7     4     4     4     7     6     5     7     4   \n",
       "4     5     5     6     6     4     4     2     5     4     5     4     4   \n",
       "\n",
       "   q136  q137  q138  q139  q141  q142  q143  q144  q145  q146  q151  q152  \\\n",
       "0     6     6     6     6     7     5     2     6     5     6     6     3   \n",
       "1     4     4     4     4     6     6     3     4     6     5     4     3   \n",
       "2     5     5     5     6     6     6     2     2     7     7     2     4   \n",
       "3     6     7     7     7     7     7     5     5     4     4     7     4   \n",
       "4     3     2     2     3     7     7     1     1     7     7     1     7   \n",
       "\n",
       "   q153  q154  q155  q156  q157  q158  q159  q1510  q71  q711  q712   q72  \\\n",
       "0     6     4     5     3     7     3     4      1  1.8   1.5   2.1  2.40   \n",
       "1     6     6     5     5     6     5     7      5  1.2   1.2   1.2  0.80   \n",
       "2     7     7     7     4     4     1     4      4  1.2   1.2   1.2  1.25   \n",
       "3     4     1     6     1     4     1     1      6  0.8   0.8   0.8  0.80   \n",
       "4     7     6     7     3     1     1     4      2  0.5   0.5   0.2  0.80   \n",
       "\n",
       "   q721  q722  q723   q73  q731  q732   q74  q741  q742  q75  q751  q91  q911  \\\n",
       "0  1.60   2.4   2.4  1.20  1.40  1.20  0.30  0.40   0.5  0.0  0.00  0.6   0.6   \n",
       "1  0.80   1.0   0.8  0.80  0.80  0.80  1.00  1.00   1.0  0.5  0.40  0.8   1.0   \n",
       "2  1.25   1.5   1.0  1.25  1.25  1.25  1.00  1.40   1.4  0.6  0.50  1.0   1.2   \n",
       "3  0.80   0.8   0.8  1.20  1.20  1.20  0.80  0.80   0.8  0.4  0.40  1.2   1.2   \n",
       "4  0.40   0.6   0.6  1.60  1.20  0.80  0.75  0.75   1.0  0.1  0.25  1.5   1.5   \n",
       "\n",
       "   q912   q92  q921  q922  q923  q924  q93  q931  q932  q933  q934  q94  q941  \\\n",
       "0   0.5  0.90  0.90  1.05  0.75  0.90  0.9  0.75  1.05   0.9  0.75  1.8   1.8   \n",
       "1   1.2  0.80  1.00  1.00  1.00  1.20  1.0  1.00  1.00   0.8  1.00  1.0   1.2   \n",
       "2   0.8  1.25  1.75  1.25  1.25  1.25  1.0  1.00  1.20   0.8  1.00  1.2   1.2   \n",
       "3   1.2  0.40  0.40  0.40  0.40  0.40  0.4  0.40  0.40   0.4  0.40  0.8   0.8   \n",
       "4   1.5  0.20  0.10  0.05  0.05  0.15  0.6  0.40  1.00   1.2  1.00  1.6   2.4   \n",
       "\n",
       "   q942  q943   q95  q951  q952  q953  attendance_count  q21_agev152  \\\n",
       "0   1.8   1.8  2.10  2.10  2.10   2.1                 7            3   \n",
       "1   1.2   1.2  1.00  1.20  1.20   1.2                 8            5   \n",
       "2   1.4   1.0  0.75  1.05  0.75   0.9                11            5   \n",
       "3   0.8   0.8  1.20  1.20  1.20   1.2                 5            3   \n",
       "4   2.4   0.4  0.60  0.70  0.70   0.7                11            6   \n",
       "\n",
       "   q22_sexv153  q25_incomev161  stay  \n",
       "0            2             4.0     0  \n",
       "1            2             4.0     0  \n",
       "2            2             6.0     1  \n",
       "3            1             6.0     0  \n",
       "4            1             5.0     1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.concat([df_avg, product_scale_7, product_scale_9, attend_sum_result, df_demograph], axis=1)\n",
    "all_df.rename({0: 'attendance_count'}, axis=1, inplace=True)\n",
    "all_df.drop(['q11_id'], axis=1, inplace=True)\n",
    "\n",
    "# >6    stay\n",
    "# <=6   churn\n",
    "all_df['stay']=(all_df['q146']>6).astype(int)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoNF40xGKuL1"
   },
   "source": [
    "# 3.1 What are the characteristics of the fans that will stay compared to those will not stay?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzLituaeQKxJ"
   },
   "source": [
    "## 3.1.1 Counts of games attendance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "zqpiyItRMJvj"
   },
   "outputs": [],
   "source": [
    "# >6    stay\n",
    "# <=6   churn\n",
    "df['stay']=(df['q146']>6).astype(int)\n",
    "df['attendance_count'] = attend_sum_result\n",
    "\n",
    "df_stay = df[df['stay'] == 1]\n",
    "df_not_stay = df[df['stay'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13646,
     "status": "ok",
     "timestamp": 1618474181568,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "_S4upWiML7wr",
    "outputId": "b11aad1e-97d0-4697-999e-e9d738cce157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      19\n",
      "1       7\n",
      "2       4\n",
      "3      10\n",
      "4       4\n",
      "5       8\n",
      "6       8\n",
      "7      12\n",
      "8      21\n",
      "9      39\n",
      "10     61\n",
      "11    173\n",
      "Name: attendance_count, dtype: int64\n",
      "0     17\n",
      "1      7\n",
      "2      4\n",
      "3      8\n",
      "4      9\n",
      "5     12\n",
      "6     16\n",
      "7     20\n",
      "8     32\n",
      "9     31\n",
      "10    59\n",
      "11    93\n",
      "Name: attendance_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "stay_attend_count = df_stay[\"attendance_count\"].value_counts().sort_index()\n",
    "not_stay_attend_count = df_not_stay[\"attendance_count\"].value_counts().sort_index()\n",
    "\n",
    "print(stay_attend_count)\n",
    "print(not_stay_attend_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 15450,
     "status": "ok",
     "timestamp": 1618474183390,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "0uSCXumemxa7",
    "outputId": "33589f82-3d62-4985-a100-908685154c52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Total Attendance Count')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xpos = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "y1 = list(stay_attend_count.values)\n",
    "y2 = list(not_stay_attend_count.values)\n",
    "\n",
    "w = 0.4 # width\n",
    "adjusted_xpos = np.add(xpos, w)\n",
    "\n",
    "plt.bar(xpos, y1, width = w, color = \"red\", label = \"Stay\",alpha=0.6)\n",
    "plt.bar(adjusted_xpos, y2, width = w, color = \"lightblue\", label = \"Not Stay\",alpha=0.9)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Total Attendance Count\")\n",
    "# plt.savefig(\"attend_bar.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvPosn7NsksS"
   },
   "source": [
    "## 3.1.2 Whom do they attend games with?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15441,
     "status": "ok",
     "timestamp": 1618474183391,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "idb3E52qSsj4",
    "outputId": "b0f904da-47ea-4b56-b0e9-687fa9f25ed4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0     19\n",
      "2.0    173\n",
      "3.0    141\n",
      "Name: q16_attd_who, dtype: int64\n",
      "1.0     15\n",
      "2.0    141\n",
      "3.0    127\n",
      "Name: q16_attd_who, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "stay_who = df_stay[\"q16_attd_who\"].value_counts().sort_index()\n",
    "not_stay_who = df_not_stay[\"q16_attd_who\"].value_counts().sort_index()\n",
    "\n",
    "print(stay_who)\n",
    "print(not_stay_who)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 15432,
     "status": "ok",
     "timestamp": 1618474183392,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "WXuyb0YaWsOR",
    "outputId": "4b390e06-1a2f-4e33-9a60-46efca6d5aa9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Attend Games With')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xpos = [0, 1, 2]\n",
    "y1 = list(stay_who.values)\n",
    "y2 = list(not_stay_who.values)\n",
    "\n",
    "w = 0.4 # width\n",
    "adjusted_xpos = np.add(xpos, w)\n",
    "\n",
    "plt.bar(xpos, y1, width = w, color = \"red\", label = \"Stay\",alpha=0.6)\n",
    "plt.bar(adjusted_xpos, y2, width = w, color = \"lightblue\", label = \"Not Stay\",alpha=0.9)\n",
    "\n",
    "x_label = [\"Alone\",\"With family\",\"With friends\"]\n",
    "plt.xticks(xpos, x_label)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Attend Games With\")\n",
    "# plt.savefig(\"who_bar.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhahu557s7U6"
   },
   "source": [
    "## 3.1.3 What is their age?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15424,
     "status": "ok",
     "timestamp": 1618474183393,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "-ACaIlZYECXu",
    "outputId": "0eccbf57-d9e9-4d62-c3ec-49a36411b954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    15\n",
      "2    28\n",
      "3    72\n",
      "4    18\n",
      "5    98\n",
      "6    88\n",
      "7    47\n",
      "Name: q21_agev152, dtype: int64\n",
      "1     7\n",
      "2    19\n",
      "3    92\n",
      "4     8\n",
      "5    92\n",
      "6    70\n",
      "7    20\n",
      "Name: q21_agev152, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "stay_age_group = df_stay[\"q21_agev152\"].value_counts().sort_index()\n",
    "not_stay_age_group = df_not_stay[\"q21_agev152\"].value_counts().sort_index()\n",
    "\n",
    "print(stay_age_group)\n",
    "print(not_stay_age_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 15414,
     "status": "ok",
     "timestamp": 1618474183394,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "FV5wk06kEx9F",
    "outputId": "a5ac67ae-ef04-4b02-c882-93a0e8281c3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Age Group')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xpos = [0, 1, 2, 3, 4, 5, 6]\n",
    "y1 = list(stay_age_group.values)\n",
    "y2 = list(not_stay_age_group.values)\n",
    "\n",
    "w = 0.4 # width\n",
    "adjusted_xpos = np.add(xpos, w)\n",
    "\n",
    "plt.bar(xpos, y1, width = w, color = \"red\", label = \"Stay\",alpha=0.6)\n",
    "plt.bar(adjusted_xpos, y2, width = w, color = \"lightblue\", label = \"Not Stay\",alpha=0.9)\n",
    "\n",
    "x_label = [\"Under 18\",\"18-24\",\"25-34\",\"35-44\",\"45-54\",\"55-64\",\"65 or over\"]\n",
    "plt.xticks(xpos, x_label)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Age Group\")\n",
    "# plt.savefig(\"age_bar.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ru7bAcBwtBbo"
   },
   "source": [
    "## 3.1.4 What is their gender?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15406,
     "status": "ok",
     "timestamp": 1618474183395,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "jTbX5iYUMfGZ",
    "outputId": "d39a6739-f37b-4805-8bcb-2793dc7a1795"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    148\n",
      "2    218\n",
      "Name: q22_sexv153, dtype: int64\n",
      "1    116\n",
      "2    192\n",
      "Name: q22_sexv153, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "stay_gender = df_stay[\"q22_sexv153\"].value_counts().sort_index()\n",
    "not_stay_gender = df_not_stay[\"q22_sexv153\"].value_counts().sort_index()\n",
    "\n",
    "print(stay_gender)\n",
    "print(not_stay_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 15398,
     "status": "ok",
     "timestamp": 1618474183396,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "M5Jzt-ISPmgb",
    "outputId": "65436ea5-325a-4eeb-eb64-2fb2cc725cac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Gender Group')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xpos = [0, 1]\n",
    "y1 = list(stay_gender.values)\n",
    "y2 = list(not_stay_gender.values)\n",
    "\n",
    "w = 0.4 # width\n",
    "adjusted_xpos = np.add(xpos, w)\n",
    "\n",
    "plt.bar(xpos, y1, width = w, color = \"red\", label = \"Stay\",alpha=0.6)\n",
    "plt.bar(adjusted_xpos, y2, width = w, color = \"lightblue\", label = \"Not Stay\",alpha=0.9)\n",
    "\n",
    "x_label = [\"Female\", \"Male\"]\n",
    "plt.xticks(xpos, x_label)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Gender Group\")\n",
    "# plt.savefig(\"gender_bar.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twqoqSHjtDmp"
   },
   "source": [
    "## 3.1.5 What is their income? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15390,
     "status": "ok",
     "timestamp": 1618474183397,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "_BQrmpT_SJwJ",
    "outputId": "91dd1c61-8f59-43fd-e568-d31d75ff1dd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0     21\n",
      "2.0     30\n",
      "3.0     45\n",
      "4.0     42\n",
      "5.0     36\n",
      "6.0    136\n",
      "7.0     52\n",
      "Name: q25_incomev161, dtype: int64\n",
      "1.0      4\n",
      "2.0     20\n",
      "3.0     24\n",
      "4.0     37\n",
      "5.0     32\n",
      "6.0    147\n",
      "7.0     38\n",
      "Name: q25_incomev161, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "stay_income = df_stay[\"q25_incomev161\"].value_counts().sort_index()\n",
    "not_stay_income = df_not_stay[\"q25_incomev161\"].value_counts().sort_index()\n",
    "\n",
    "print(stay_income)\n",
    "print(not_stay_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 15378,
     "status": "ok",
     "timestamp": 1618474183398,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "mkiWrDkiP9dA",
    "outputId": "1f2039e3-e8d9-4edd-83cf-6236224d7e70"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Income Group')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xpos = [0,1,2,3,4,5,6]\n",
    "y1 = list(stay_income.values)\n",
    "y2 = list(not_stay_income.values)\n",
    "\n",
    "w = 0.4 # width\n",
    "adjusted_xpos = np.add(xpos, w)\n",
    "\n",
    "plt.bar(xpos, y1, width = w, color = \"red\", label = \"Stay\",alpha=0.6)\n",
    "plt.bar(adjusted_xpos, y2, width = w, color = \"lightblue\", label = \"Not Stay\",alpha=0.9)\n",
    "\n",
    "x_label = [\"Under$25k\",\"$25k+\",\"$41k+\",\"$56k+\",\"$70k\",\"Over $85k\",\"NA\"]\n",
    "plt.xticks(xpos, x_label)\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Income Group\")\n",
    "# plt.savefig(\"income_bar.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tu8UU45SdcXM"
   },
   "source": [
    "# 3.2 Which hypothetical situations can influence fans' decision to stay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "executionInfo": {
     "elapsed": 185143,
     "status": "ok",
     "timestamp": 1618474511926,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "ldSmSOizdcXN",
    "outputId": "5a06faad-2138-49b0-da76-dfdbc7cb7440"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q151</th>\n",
       "      <th>q152</th>\n",
       "      <th>q153</th>\n",
       "      <th>q154</th>\n",
       "      <th>q155</th>\n",
       "      <th>q156</th>\n",
       "      <th>q157</th>\n",
       "      <th>q158</th>\n",
       "      <th>q159</th>\n",
       "      <th>q1510</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>674 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     q151  q152  q153  q154  q155  q156  q157  q158  q159  q1510\n",
       "0       6     3     6     4     5     3     7     3     4      1\n",
       "1       4     3     6     6     5     5     6     5     7      5\n",
       "2       2     4     7     7     7     4     4     1     4      4\n",
       "3       7     4     4     1     6     1     4     1     1      6\n",
       "4       1     7     7     6     7     3     1     1     4      2\n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...    ...\n",
       "669     4     5     7     7     7     7     6     4     7      7\n",
       "670     4     1     7     7     2     4     7     1     7      4\n",
       "671     4     7     7     7     7     2     2     2     7      2\n",
       "672     5     4     7     7     5     4     6     6     7      4\n",
       "673     5     7     7     7     7     4     7     7     7      4\n",
       "\n",
       "[674 rows x 10 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Q15=all_df[['q151','q152','q153','q154','q155','q156','q157','q158','q159','q1510']]\n",
    "y_Q15=all_df['stay']\n",
    "X_Q15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I76iAwzAdcXO"
   },
   "source": [
    "## 3.2.1 Logit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "wyblqHtPdcXP"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_Q15, y_Q15, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 185133,
     "status": "ok",
     "timestamp": 1618474511927,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "-9FuTKjGdcXP",
    "outputId": "7fe4ad35-29f8-421c-fd6e-ebf49a77debc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "log_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 185130,
     "status": "ok",
     "timestamp": 1618474511929,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "7y1yJ6lqdcXQ",
    "outputId": "53b33bd1-0aba-4224-be16-e899c686040d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.01960754]\n",
      "[[-0.04008457  0.26453947  0.6484034   0.05270501  0.20562111  0.13607085\n",
      "   0.04644253 -0.01181407  0.05583019  0.03902303]]\n"
     ]
    }
   ],
   "source": [
    "print(log_reg.intercept_)\n",
    "print(log_reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 185547,
     "status": "ok",
     "timestamp": 1618474512351,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "zPnOFoMwdcXQ",
    "outputId": "d933217d-e377-47d8-bc68-f1beddf30d52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 5.39255744e-01 3.05325056e-03 1.77333703e-11\n",
      " 6.29620593e-01 6.52199370e-02 8.26580992e-02 4.65743710e-01\n",
      " 8.56409924e-01 6.48046895e-01 5.47369162e-01]\n"
     ]
    }
   ],
   "source": [
    "# Get P_values\n",
    "from scipy.stats import norm\n",
    "\n",
    "def logit_pvalue(model, x):\n",
    "    p = model.predict_proba(x)\n",
    "    n = len(p)\n",
    "    m = len(model.coef_[0]) + 1\n",
    "    coefs = np.concatenate([model.intercept_, model.coef_[0]])\n",
    "    x_full = np.matrix(np.insert(np.array(x), 0, 1, axis = 1))\n",
    "    ans = np.zeros((m, m))\n",
    "    for i in range(n):\n",
    "        ans = ans + np.dot(np.transpose(x_full[i, :]), x_full[i, :]) * p[i,1] * p[i, 0]\n",
    "    vcov = np.linalg.inv(np.matrix(ans))\n",
    "    se = np.sqrt(np.diag(vcov))\n",
    "    t =  coefs/se  \n",
    "    p = (1 - norm.cdf(abs(t))) * 2\n",
    "    return p\n",
    "\n",
    "pvalue = logit_pvalue(log_reg, x_train)\n",
    "print(pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "executionInfo": {
     "elapsed": 185543,
     "status": "ok",
     "timestamp": 1618474512352,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "nbju8WModcXQ",
    "outputId": "81b8f38f-3148-44e1-a3d2-998e97fffe7a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>p_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>q153</th>\n",
       "      <td>0.648403</td>\n",
       "      <td>1.773337e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q152</th>\n",
       "      <td>0.264539</td>\n",
       "      <td>3.053251e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q155</th>\n",
       "      <td>0.205621</td>\n",
       "      <td>6.521994e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q156</th>\n",
       "      <td>0.136071</td>\n",
       "      <td>8.265810e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q159</th>\n",
       "      <td>0.055830</td>\n",
       "      <td>6.480469e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q154</th>\n",
       "      <td>0.052705</td>\n",
       "      <td>6.296206e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q157</th>\n",
       "      <td>0.046443</td>\n",
       "      <td>4.657437e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q1510</th>\n",
       "      <td>0.039023</td>\n",
       "      <td>5.473692e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q158</th>\n",
       "      <td>-0.011814</td>\n",
       "      <td>8.564099e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q151</th>\n",
       "      <td>-0.040085</td>\n",
       "      <td>5.392557e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>-7.019608</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           coefficient         p_val\n",
       "q153          0.648403  1.773337e-11\n",
       "q152          0.264539  3.053251e-03\n",
       "q155          0.205621  6.521994e-02\n",
       "q156          0.136071  8.265810e-02\n",
       "q159          0.055830  6.480469e-01\n",
       "q154          0.052705  6.296206e-01\n",
       "q157          0.046443  4.657437e-01\n",
       "q1510         0.039023  5.473692e-01\n",
       "q158         -0.011814  8.564099e-01\n",
       "q151         -0.040085  5.392557e-01\n",
       "Intercept    -7.019608  0.000000e+00"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_a=log_reg.intercept_\n",
    "lst_b=log_reg.coef_\n",
    "X_column = X_Q15.columns.tolist()\n",
    "\n",
    "lst=np.concatenate((lst_a, lst_b[0]))\n",
    "\n",
    "lst2=logit_pvalue(log_reg,x_train)\n",
    "\n",
    "data = pd.DataFrame(list(zip(lst, lst2)), \n",
    "              columns =['coefficient', 'p_val'],\n",
    "              index = ['Intercept'] + X_column) \n",
    "\n",
    "# from largest to smallest\n",
    "data.sort_values('coefficient', axis = 0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 185540,
     "status": "ok",
     "timestamp": 1618474512354,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "AxUhgy0GdcXR",
    "outputId": "a0ef1de3-24ce-4ef1-a128-3f1f9fd379e9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>p_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>-7.019608</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q152</th>\n",
       "      <td>0.264539</td>\n",
       "      <td>3.053251e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q153</th>\n",
       "      <td>0.648403</td>\n",
       "      <td>1.773337e-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           coefficient         p_val\n",
       "Intercept    -7.019608  0.000000e+00\n",
       "q152          0.264539  3.053251e-03\n",
       "q153          0.648403  1.773337e-11"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# significant\n",
    "data[data['p_val'] < 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "executionInfo": {
     "elapsed": 185536,
     "status": "ok",
     "timestamp": 1618474512355,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "SlJvbGAMdcXR",
    "outputId": "abdcfe7e-d966-458e-d2d2-df64bbb47782"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic</th>\n",
       "      <td>0.242604</td>\n",
       "      <td>0.793814</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.502818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Error Rate  Sensitivity  Specificity  Precision       AUC  \\\n",
       "Logistic    0.242604     0.793814     0.708333   0.785714  0.833333   \n",
       "\n",
       "          Cross-Entropy  \n",
       "Logistic       0.502818  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_score, log_loss\n",
    "\n",
    "columns=['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows=['Logistic']\n",
    "\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "y_pred = log_reg.predict(x_test) \n",
    "y_prob = log_reg.predict_proba(x_test)[:,1]\n",
    "\n",
    "confusion  = confusion_matrix(y_test, y_pred) \n",
    "results.iloc[0,0]=  1 - accuracy_score(y_test, y_pred)\n",
    "results.iloc[0,1]=  confusion[1,1]/np.sum(confusion[1,:])\n",
    "results.iloc[0,2]=  confusion[0,0]/np.sum(confusion[0,:])\n",
    "results.iloc[0,3]=  precision_score(y_test, y_pred)\n",
    "\n",
    "results.iloc[0, 4]=  roc_auc_score(y_test, y_prob)   \n",
    "results.iloc[0, 5]=  log_loss(y_test, y_prob)   \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "8X__wZHldcXR"
   },
   "outputs": [],
   "source": [
    "# y_comparison=pd.DataFrame({'y_test':y_test,\n",
    "#                           'y_pred': y_pred})\n",
    "\n",
    "# pd.DataFrame(y_comparison).to_csv('y_comparison3.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wPEH5aMdcXS"
   },
   "source": [
    "## 3.2.2 Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 186416,
     "status": "ok",
     "timestamp": 1618474513248,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "AUYyRgNXdcXS",
    "outputId": "963687b2-4fbe-442f-90e4-48912cdf2b92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=16, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 186818,
     "status": "ok",
     "timestamp": 1618474513659,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "UDlrJO87dcXS",
    "outputId": "9be2bc3c-ff9e-4dbf-e9a7-c48eccc93619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04090143 0.11861818 0.33725535 0.04077085 0.15755788 0.07949551\n",
      " 0.11276588 0.03601057 0.05005512 0.02656923]\n"
     ]
    }
   ],
   "source": [
    "print(rnd.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "executionInfo": {
     "elapsed": 186811,
     "status": "ok",
     "timestamp": 1618474513663,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "2KTH6lHXdcXT",
    "outputId": "d45b6f7f-db17-427d-9dfb-9e40bdbdee31"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>q153</th>\n",
       "      <td>0.337255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q155</th>\n",
       "      <td>0.157558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q152</th>\n",
       "      <td>0.118618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q157</th>\n",
       "      <td>0.112766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q156</th>\n",
       "      <td>0.079496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q159</th>\n",
       "      <td>0.050055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q151</th>\n",
       "      <td>0.040901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q154</th>\n",
       "      <td>0.040771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q158</th>\n",
       "      <td>0.036011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q1510</th>\n",
       "      <td>0.026569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature_importance\n",
       "q153             0.337255\n",
       "q155             0.157558\n",
       "q152             0.118618\n",
       "q157             0.112766\n",
       "q156             0.079496\n",
       "q159             0.050055\n",
       "q151             0.040901\n",
       "q154             0.040771\n",
       "q158             0.036011\n",
       "q1510            0.026569"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(rnd.feature_importances_, \n",
    "              columns = ['feature_importance'],\n",
    "              index = X_column) \n",
    "\n",
    "# from largest to smallest\n",
    "data.sort_values('feature_importance', axis = 0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "executionInfo": {
     "elapsed": 187282,
     "status": "ok",
     "timestamp": 1618474514149,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "1Y70g6qGdcXT",
    "outputId": "f3774d2f-d595-41ab-eaca-382cc28f04c4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.242604</td>\n",
       "      <td>0.773196</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.797872</td>\n",
       "      <td>0.836483</td>\n",
       "      <td>0.499165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Error Rate  Sensitivity  Specificity  Precision       AUC  \\\n",
       "Random Forest    0.242604     0.773196     0.736111   0.797872  0.836483   \n",
       "\n",
       "               Cross-Entropy  \n",
       "Random Forest       0.499165  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_score, log_loss\n",
    "\n",
    "columns=['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows=['Random Forest']\n",
    "\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "y_pred = rnd.predict(x_test) \n",
    "y_prob = rnd.predict_proba(x_test)[:,1]\n",
    "\n",
    "confusion  = confusion_matrix(y_test, y_pred) \n",
    "results.iloc[0,0]=  1 - accuracy_score(y_test, y_pred)\n",
    "results.iloc[0,1]=  confusion[1,1]/np.sum(confusion[1,:])\n",
    "results.iloc[0,2]=  confusion[0,0]/np.sum(confusion[0,:])\n",
    "results.iloc[0,3]=  precision_score(y_test, y_pred)\n",
    "\n",
    "results.iloc[0, 4]=  roc_auc_score(y_test, y_prob)   \n",
    "results.iloc[0, 5]=  log_loss(y_test, y_prob)   \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEFip5gZdcXT"
   },
   "source": [
    "## 3.2.3 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260180,
     "status": "ok",
     "timestamp": 1618474587055,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "nwpmi7rYdcXT",
    "outputId": "042a66dc-3ebd-4084-92da-adfd1d64730d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:38] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { min_samples_leaf, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Best parameters found: {'n_estimators': 2000, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_depth': 1, 'learning_rate': 0.005}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model = XGBClassifier(reg_lambda=0)\n",
    "\n",
    "space = {\n",
    "    'n_estimators': [100, 300, 500, 1000, 2000],\n",
    "    'learning_rate': [0.005, 0.15],\n",
    "    'max_depth': [1, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10]\n",
    "}\n",
    "#np.random.seed(87)\n",
    "#xgb_search =  BayesSearchCV(model, space, cv = 5,  n_iter=32, scoring = 'neg_log_loss', n_jobs=4)\n",
    "xgb_search =  RandomizedSearchCV(model, space, cv = 5,  n_iter=32, scoring = 'neg_root_mean_squared_error', n_jobs=4)\n",
    "\n",
    "xgb_search.fit(x_train, y_train)\n",
    "\n",
    "XGB_clf = xgb_search.best_estimator_\n",
    "\n",
    "print('Best parameters found:', xgb_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260173,
     "status": "ok",
     "timestamp": 1618474587056,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "D7a3I7QqdcXU",
    "outputId": "6400b966-e598-46bf-e8ac-0c694d7e319e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01427367 0.10696913 0.43087348 0.         0.2295484  0.05857681\n",
      " 0.04981449 0.01992354 0.07527336 0.01474708]\n"
     ]
    }
   ],
   "source": [
    "print(XGB_clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "executionInfo": {
     "elapsed": 260166,
     "status": "ok",
     "timestamp": 1618474587057,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "TlkgezX5dcXU",
    "outputId": "9bd95a15-2b77-46ed-dc43-dc48203d3323"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>q153</th>\n",
       "      <td>0.430873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q155</th>\n",
       "      <td>0.229548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q152</th>\n",
       "      <td>0.106969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q159</th>\n",
       "      <td>0.075273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q156</th>\n",
       "      <td>0.058577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q157</th>\n",
       "      <td>0.049814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q158</th>\n",
       "      <td>0.019924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q1510</th>\n",
       "      <td>0.014747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q151</th>\n",
       "      <td>0.014274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q154</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature_importance\n",
       "q153             0.430873\n",
       "q155             0.229548\n",
       "q152             0.106969\n",
       "q159             0.075273\n",
       "q156             0.058577\n",
       "q157             0.049814\n",
       "q158             0.019924\n",
       "q1510            0.014747\n",
       "q151             0.014274\n",
       "q154             0.000000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(XGB_clf.feature_importances_, \n",
    "              columns = ['feature_importance'],\n",
    "              index = X_column) \n",
    "\n",
    "# from largest to smallest\n",
    "data.sort_values('feature_importance', axis = 0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "executionInfo": {
     "elapsed": 260162,
     "status": "ok",
     "timestamp": 1618474587058,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "sSZE3NdydcXU",
    "outputId": "2cc40eb2-2883-4b15-d7a8-93b215a2ae3b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XG Boost</th>\n",
       "      <td>0.260355</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.837772</td>\n",
       "      <td>0.501509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Error Rate  Sensitivity  Specificity  Precision       AUC  \\\n",
       "XG Boost    0.260355     0.721649     0.763889   0.804598  0.837772   \n",
       "\n",
       "          Cross-Entropy  \n",
       "XG Boost       0.501509  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_score, log_loss\n",
    "\n",
    "columns=['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows=['XG Boost']\n",
    "\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "y_pred = XGB_clf.predict(x_test) \n",
    "y_prob = XGB_clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "confusion  = confusion_matrix(y_test, y_pred) \n",
    "results.iloc[0,0]=  1 - accuracy_score(y_test, y_pred)\n",
    "results.iloc[0,1]=  confusion[1,1]/np.sum(confusion[1,:])\n",
    "results.iloc[0,2]=  confusion[0,0]/np.sum(confusion[0,:])\n",
    "results.iloc[0,3]=  precision_score(y_test, y_pred)\n",
    "\n",
    "results.iloc[0, 4]=  roc_auc_score(y_test, y_prob)     \n",
    "results.iloc[0, 5]=  log_loss(y_test, y_prob)   \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRwsXUpkdcXW"
   },
   "source": [
    "## 3.2.4 Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260158,
     "status": "ok",
     "timestamp": 1618474587059,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "gBY2O-aldcXX",
    "outputId": "9ff6740a-13be-4a3b-e025-f62bf46728c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=False, with_std=True)),\n",
       "                ('linear_svc',\n",
       "                 LinearSVC(C=1, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='hinge', max_iter=1000000, multi_class='ovr',\n",
       "                           penalty='l2', random_state=None, tol=0.0001,\n",
       "                           verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "SVM_clf = Pipeline([\n",
    "         (\"scaler\", StandardScaler(with_mean=False)), \n",
    "         (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\",max_iter=1000000)), \n",
    "     ])\n",
    "\n",
    "SVM_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260147,
     "status": "ok",
     "timestamp": 1618474587060,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "utcXeOygdcXX",
    "outputId": "89ca7aea-c970-4a65-e46a-9e7b12b655d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=10000000, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm = svm.SVC(kernel='linear',max_iter=10000000)\n",
    "svm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "executionInfo": {
     "elapsed": 260143,
     "status": "ok",
     "timestamp": 1618474587062,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "dsZHSIwQdcXX",
    "outputId": "cec86802-1653-4fce-981c-3ab369ea09e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q153</td>\n",
       "      <td>0.648902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q155</td>\n",
       "      <td>0.169224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q152</td>\n",
       "      <td>0.161382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>q156</td>\n",
       "      <td>0.103450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q151</td>\n",
       "      <td>-0.047859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>q1510</td>\n",
       "      <td>0.039842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>q159</td>\n",
       "      <td>0.032440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q154</td>\n",
       "      <td>0.016278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>q158</td>\n",
       "      <td>0.010027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>q157</td>\n",
       "      <td>-0.009566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       X  feature_importance\n",
       "2   q153            0.648902\n",
       "4   q155            0.169224\n",
       "1   q152            0.161382\n",
       "5   q156            0.103450\n",
       "0   q151           -0.047859\n",
       "9  q1510            0.039842\n",
       "8   q159            0.032440\n",
       "3   q154            0.016278\n",
       "7   q158            0.010027\n",
       "6   q157           -0.009566"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = svm.coef_\n",
    "\n",
    "data = pd.DataFrame({\"X\": X_column, \"feature_importance\": feature_importance[0]})\n",
    "\n",
    "# from largest to smallest - absolute value\n",
    "data.sort_values('feature_importance', axis = 0, ascending = False, key = abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "executionInfo": {
     "elapsed": 260140,
     "status": "ok",
     "timestamp": 1618474587064,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "oUX-N40hdcXX",
    "outputId": "667f4c32-798b-4747-e5d1-fb738aa7a6e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.236686</td>\n",
       "      <td>0.804124</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.82732</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Error Rate  Sensitivity  Specificity  Precision      AUC  Cross-Entropy\n",
       "SVM    0.236686     0.804124     0.708333   0.787879  0.82732            0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_score, log_loss\n",
    "\n",
    "columns=['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows=['SVM']\n",
    "\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "y_pred = svm.predict(x_test) \n",
    "#y_prob = SVM_clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "confusion  = confusion_matrix(y_test, y_pred) \n",
    "results.iloc[0,0]=  1 - accuracy_score(y_test, y_pred)\n",
    "results.iloc[0,1]=  confusion[1,1]/np.sum(confusion[1,:])\n",
    "results.iloc[0,2]=  confusion[0,0]/np.sum(confusion[0,:])\n",
    "results.iloc[0,3]=  precision_score(y_test, y_pred)\n",
    "y_df = svm.decision_function(x_test)\n",
    "results.iloc[0, 4]=  roc_auc_score(y_test, y_df)   \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "K0lgRzGxdcXY",
    "outputId": "c5c89b87-f41f-4c4d-98a4-73b4117c9af6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logit Model</th>\n",
       "      <td>0.242604</td>\n",
       "      <td>0.793814</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.502818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forests</th>\n",
       "      <td>0.242604</td>\n",
       "      <td>0.773196</td>\n",
       "      <td>0.736111</td>\n",
       "      <td>0.797872</td>\n",
       "      <td>0.836483</td>\n",
       "      <td>0.499165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.260355</td>\n",
       "      <td>0.721649</td>\n",
       "      <td>0.763889</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.837772</td>\n",
       "      <td>0.501509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.236686</td>\n",
       "      <td>0.804124</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.827320</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Error Rate  Sensitivity  Specificity  Precision       AUC  \\\n",
       "Logit Model          0.242604     0.793814     0.708333   0.785714  0.833333   \n",
       "Random Forests       0.242604     0.773196     0.736111   0.797872  0.836483   \n",
       "Gradient Boosting    0.260355     0.721649     0.763889   0.804598  0.837772   \n",
       "SVM                  0.236686     0.804124     0.708333   0.787879  0.827320   \n",
       "\n",
       "                   Cross-Entropy  \n",
       "Logit Model             0.502818  \n",
       "Random Forests          0.499165  \n",
       "Gradient Boosting       0.501509  \n",
       "SVM                     0.000000  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, precision_score, log_loss\n",
    "\n",
    "columns = ['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows = ['Logit Model', 'Random Forests', 'Gradient Boosting', 'SVM']\n",
    "results = pd.DataFrame(0.0, columns = columns, index = rows) \n",
    "\n",
    "methods = [log_reg, rnd, XGB_clf, svm]\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    if method != svm:\n",
    "        y_pred = method.predict(x_test) \n",
    "        y_prob = method.predict_proba(x_test)[:,1]\n",
    "        confusion = confusion_matrix(y_test, y_pred) \n",
    "        results.iloc[i,0] = 1 - accuracy_score(y_test, y_pred)\n",
    "        results.iloc[i,1] = confusion[1,1]/np.sum(confusion[1,:])\n",
    "        results.iloc[i,2] = confusion[0,0]/np.sum(confusion[0,:])\n",
    "        results.iloc[i,3] = precision_score(y_test, y_pred)\n",
    "        results.iloc[i,4] = roc_auc_score(y_test, y_prob)   \n",
    "        results.iloc[i,5] = log_loss(y_test, y_prob)\n",
    "    else:\n",
    "        y_pred = method.predict(x_test) \n",
    "        confusion = confusion_matrix(y_test, y_pred) \n",
    "        results.iloc[i,0] = 1 - accuracy_score(y_test, y_pred)\n",
    "        results.iloc[i,1] = confusion[1,1]/np.sum(confusion[1,:])\n",
    "        results.iloc[i,2] = confusion[0,0]/np.sum(confusion[0,:])\n",
    "        results.iloc[i,3] = precision_score(y_test, y_pred)\n",
    "        y_df = svm.decision_function(x_test)\n",
    "        results.iloc[i,4] = roc_auc_score(y_test, y_df) \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TL948QketGpW"
   },
   "source": [
    "# 3.3 How does fans love influence their decision to stay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "executionInfo": {
     "elapsed": 15368,
     "status": "ok",
     "timestamp": 1618474183399,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "qfF3QHlQtGpW",
    "outputId": "7418d057-432c-4ff8-d718-8db1620ba62a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q15_team_sup</th>\n",
       "      <th>q18_game_last</th>\n",
       "      <th>attendance_count</th>\n",
       "      <th>q3</th>\n",
       "      <th>q21</th>\n",
       "      <th>q22</th>\n",
       "      <th>q23</th>\n",
       "      <th>q24</th>\n",
       "      <th>q25</th>\n",
       "      <th>q26</th>\n",
       "      <th>q27</th>\n",
       "      <th>q28</th>\n",
       "      <th>q29</th>\n",
       "      <th>q210</th>\n",
       "      <th>q211</th>\n",
       "      <th>q51</th>\n",
       "      <th>q52</th>\n",
       "      <th>q53</th>\n",
       "      <th>q54</th>\n",
       "      <th>q55</th>\n",
       "      <th>q56</th>\n",
       "      <th>q57</th>\n",
       "      <th>q58</th>\n",
       "      <th>q131</th>\n",
       "      <th>q132</th>\n",
       "      <th>q133</th>\n",
       "      <th>q134</th>\n",
       "      <th>q135</th>\n",
       "      <th>q136</th>\n",
       "      <th>q137</th>\n",
       "      <th>q138</th>\n",
       "      <th>q139</th>\n",
       "      <th>q141</th>\n",
       "      <th>q142</th>\n",
       "      <th>q143</th>\n",
       "      <th>q144</th>\n",
       "      <th>q145</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>674 rows  37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     q15_team_sup  q18_game_last  attendance_count   q3  q21  q22  q23  q24  \\\n",
       "0             2.0            1.0                 7  5.0    6    1    7  7.0   \n",
       "1             1.0            1.0                 8  4.0    1    1    6  7.0   \n",
       "2             2.0            2.0                11  7.0    3    1    5  5.0   \n",
       "3             1.0            1.0                 5  4.0    1    1    7  7.0   \n",
       "4             1.0            2.0                11  7.0    7    4    4  7.0   \n",
       "..            ...            ...               ...  ...  ...  ...  ...  ...   \n",
       "669           3.0            1.0                11  7.0    7    7    7  7.0   \n",
       "670           1.0            1.0                 3  4.0    5    5    6  6.0   \n",
       "671           2.0            1.0                 5  4.0    7    7    7  7.0   \n",
       "672           1.0            1.0                10  5.0    7    7    5  7.0   \n",
       "673           1.0            2.0                 0  7.0    7    5    6  7.0   \n",
       "\n",
       "     q25  q26  q27  q28  q29  q210  q211  q51  q52  q53  q54  q55  q56  q57  \\\n",
       "0    1.0  7.0    2    5  7.0   2.0   2.0    2    4    5    5    1    1    1   \n",
       "1    4.0  6.0    4    5  4.0   1.0   7.0    5    5    7    7    5    5    4   \n",
       "2    5.0  4.0    5    5  5.0   4.0   4.0    7    5    7    7    5    5    4   \n",
       "3    1.0  4.0    1    7  7.0   1.0   4.0    1    1    4    4    1    1    1   \n",
       "4    7.0  2.0    6    7  7.0   2.0   7.0    7    7    7    7    4    6    1   \n",
       "..   ...  ...  ...  ...  ...   ...   ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "669  5.0  4.0    5    1  7.0   1.0   7.0    7    5    7    7    5    2    2   \n",
       "670  1.0  1.0    5    1  4.0   1.0   1.0    4    3    7    6    5    4    4   \n",
       "671  4.0  7.0    7    1  2.0   1.0   3.0    6    5    7    7    7    7    4   \n",
       "672  5.0  3.0    5    1  5.0   2.0   2.0    5    5    7    6    4    4    1   \n",
       "673  4.0  7.0    7    7  7.0   3.0   7.0    7    7    7    7    4    4    5   \n",
       "\n",
       "     q58  q131  q132  q133  q134  q135  q136  q137  q138  q139  q141  q142  \\\n",
       "0      1     7     7     6     5     6     6     6     6     6     7     5   \n",
       "1      6     4     4     4     4     3     4     4     4     4     6     6   \n",
       "2      4     6     6     5     6     6     5     5     5     6     6     6   \n",
       "3      1     7     6     5     7     4     6     7     7     7     7     7   \n",
       "4      5     5     4     5     4     4     3     2     2     3     7     7   \n",
       "..   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "669    3     7     5     6     4     6     6     6     5     6     7     6   \n",
       "670    6     6     6     5     5     6     6     7     5     6     7     7   \n",
       "671    4     5     5     6     5     4     5     5     5     6     7     7   \n",
       "672    3     5     6     6     6     5     7     7     7     7     7     7   \n",
       "673    5     7     7     5     7     6     7     7     7     7     7     6   \n",
       "\n",
       "     q143  q144  q145  \n",
       "0       2     6     5  \n",
       "1       3     4     6  \n",
       "2       2     2     7  \n",
       "3       5     5     4  \n",
       "4       1     1     7  \n",
       "..    ...   ...   ...  \n",
       "669     2     7     7  \n",
       "670     5     5     6  \n",
       "671     2     4     7  \n",
       "672     6     3     6  \n",
       "673     1     4     7  \n",
       "\n",
       "[674 rows x 37 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_love=all_df[['q15_team_sup', 'q18_game_last', 'attendance_count', 'q3', \n",
    "               'q21', 'q22', 'q23', 'q24', 'q25', 'q26', 'q27', 'q28', 'q29', 'q210', 'q211', # q2\n",
    "               'q51', 'q52', 'q53', 'q54', 'q55', 'q56', 'q57', 'q58', # q5\n",
    "               'q131', 'q132', 'q133', 'q134', 'q135', 'q136', 'q137', 'q138', 'q139', # q13\n",
    "               'q141', 'q142', 'q143', 'q144', 'q145']] # q14\n",
    "               \n",
    "y_love=all_df['stay']\n",
    "X_love"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghd68uLYtGpX"
   },
   "source": [
    "## 3.3.1 Logit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "w2FvDzjqtGpX"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_love, y_love, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16991,
     "status": "ok",
     "timestamp": 1618474185033,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "W2E6Wz3ZtGpb",
    "outputId": "d79ebe03-79be-4cca-8da8-ee9f6f232fe0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "log_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16982,
     "status": "ok",
     "timestamp": 1618474185034,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "Q8C7zeDMtGpb",
    "outputId": "c53724a3-8fdc-42c0-a081-ecb6ee75f770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.40555433]\n",
      "[[-0.16964477  0.39971526  0.09280454  0.01413414 -0.01498339 -0.07214605\n",
      "  -0.03314019  0.22703042  0.12730507  0.09540592 -0.00145613  0.00990166\n",
      "  -0.00724449 -0.18274165  0.00192013  0.25467324  0.34573037 -0.48917668\n",
      "   0.47998464  0.02677528 -0.04924796  0.00194375  0.10526125 -0.03800161\n",
      "   0.06373237 -0.23366091  0.53120143  0.13145394  0.2893724   0.10632313\n",
      "  -0.24130986 -0.04136554  0.21189419 -0.12657521 -0.25829576 -0.09104258\n",
      "   0.24650934]]\n"
     ]
    }
   ],
   "source": [
    "print(log_reg.intercept_)\n",
    "print(log_reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16973,
     "status": "ok",
     "timestamp": 1618474185035,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "xn7wHteqtGpc",
    "outputId": "0e7c950d-eeee-435e-ed84-b015fe6a9eaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.13136272e-09 4.32845457e-01 1.47297646e-01 3.09761756e-02\n",
      " 8.99510901e-01 8.62121724e-01 2.27473038e-01 7.32006256e-01\n",
      " 6.57785026e-02 4.98290684e-02 1.95675632e-01 9.84917587e-01\n",
      " 8.62248943e-01 9.43319217e-01 4.09828140e-02 9.80391803e-01\n",
      " 7.55842175e-02 1.39295669e-02 2.59547520e-03 5.80014837e-03\n",
      " 7.99529491e-01 6.08962207e-01 9.84829837e-01 2.48734430e-01\n",
      " 8.35946785e-01 7.51660778e-01 7.83279015e-02 7.55134344e-03\n",
      " 1.34894517e-01 1.59159047e-01 6.04833296e-01 2.80507540e-01\n",
      " 8.33515054e-01 1.01518559e-01 3.81606020e-01 4.35739373e-04\n",
      " 2.76152608e-01 4.04272587e-02]\n"
     ]
    }
   ],
   "source": [
    "# Get P_values\n",
    "from scipy.stats import norm\n",
    "\n",
    "def logit_pvalue(model, x):\n",
    "    p = model.predict_proba(x)\n",
    "    n = len(p)\n",
    "    m = len(model.coef_[0]) + 1\n",
    "    coefs = np.concatenate([model.intercept_, model.coef_[0]])\n",
    "    x_full = np.matrix(np.insert(np.array(x), 0, 1, axis = 1))\n",
    "    ans = np.zeros((m, m))\n",
    "    for i in range(n):\n",
    "        ans = ans + np.dot(np.transpose(x_full[i, :]), x_full[i, :]) * p[i,1] * p[i, 0]\n",
    "    vcov = np.linalg.inv(np.matrix(ans))\n",
    "    se = np.sqrt(np.diag(vcov))\n",
    "    t =  coefs/se  \n",
    "    p = (1 - norm.cdf(abs(t))) * 2\n",
    "    return p\n",
    "\n",
    "pvalue = logit_pvalue(log_reg, x_train)\n",
    "print(pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 16964,
     "status": "ok",
     "timestamp": 1618474185036,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "6rMuyZURtGpc",
    "outputId": "aca574a2-f91f-404c-fdf7-a5b142a1b76f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>p_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>q134</th>\n",
       "      <td>0.531201</td>\n",
       "      <td>7.551343e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q54</th>\n",
       "      <td>0.479985</td>\n",
       "      <td>5.800148e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q18_game_last</th>\n",
       "      <td>0.399715</td>\n",
       "      <td>1.472976e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q52</th>\n",
       "      <td>0.345730</td>\n",
       "      <td>1.392957e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q136</th>\n",
       "      <td>0.289372</td>\n",
       "      <td>1.591590e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q51</th>\n",
       "      <td>0.254673</td>\n",
       "      <td>7.558422e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q145</th>\n",
       "      <td>0.246509</td>\n",
       "      <td>4.042726e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q24</th>\n",
       "      <td>0.227030</td>\n",
       "      <td>6.577850e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q141</th>\n",
       "      <td>0.211894</td>\n",
       "      <td>1.015186e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q135</th>\n",
       "      <td>0.131454</td>\n",
       "      <td>1.348945e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q25</th>\n",
       "      <td>0.127305</td>\n",
       "      <td>4.982907e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q137</th>\n",
       "      <td>0.106323</td>\n",
       "      <td>6.048333e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q58</th>\n",
       "      <td>0.105261</td>\n",
       "      <td>2.487344e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q26</th>\n",
       "      <td>0.095406</td>\n",
       "      <td>1.956756e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attendance_count</th>\n",
       "      <td>0.092805</td>\n",
       "      <td>3.097618e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q132</th>\n",
       "      <td>0.063732</td>\n",
       "      <td>7.516608e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q55</th>\n",
       "      <td>0.026775</td>\n",
       "      <td>7.995295e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q3</th>\n",
       "      <td>0.014134</td>\n",
       "      <td>8.995109e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q28</th>\n",
       "      <td>0.009902</td>\n",
       "      <td>8.622489e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q57</th>\n",
       "      <td>0.001944</td>\n",
       "      <td>9.848298e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q211</th>\n",
       "      <td>0.001920</td>\n",
       "      <td>9.803918e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q27</th>\n",
       "      <td>-0.001456</td>\n",
       "      <td>9.849176e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q29</th>\n",
       "      <td>-0.007244</td>\n",
       "      <td>9.433192e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q21</th>\n",
       "      <td>-0.014983</td>\n",
       "      <td>8.621217e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q23</th>\n",
       "      <td>-0.033140</td>\n",
       "      <td>7.320063e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q131</th>\n",
       "      <td>-0.038002</td>\n",
       "      <td>8.359468e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q139</th>\n",
       "      <td>-0.041366</td>\n",
       "      <td>8.335151e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q56</th>\n",
       "      <td>-0.049248</td>\n",
       "      <td>6.089622e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q22</th>\n",
       "      <td>-0.072146</td>\n",
       "      <td>2.274730e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q144</th>\n",
       "      <td>-0.091043</td>\n",
       "      <td>2.761526e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q142</th>\n",
       "      <td>-0.126575</td>\n",
       "      <td>3.816060e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q15_team_sup</th>\n",
       "      <td>-0.169645</td>\n",
       "      <td>4.328455e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q210</th>\n",
       "      <td>-0.182742</td>\n",
       "      <td>4.098281e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q133</th>\n",
       "      <td>-0.233661</td>\n",
       "      <td>7.832790e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q138</th>\n",
       "      <td>-0.241310</td>\n",
       "      <td>2.805075e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q143</th>\n",
       "      <td>-0.258296</td>\n",
       "      <td>4.357394e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q53</th>\n",
       "      <td>-0.489177</td>\n",
       "      <td>2.595475e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>-9.405554</td>\n",
       "      <td>3.131363e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  coefficient         p_val\n",
       "q134                 0.531201  7.551343e-03\n",
       "q54                  0.479985  5.800148e-03\n",
       "q18_game_last        0.399715  1.472976e-01\n",
       "q52                  0.345730  1.392957e-02\n",
       "q136                 0.289372  1.591590e-01\n",
       "q51                  0.254673  7.558422e-02\n",
       "q145                 0.246509  4.042726e-02\n",
       "q24                  0.227030  6.577850e-02\n",
       "q141                 0.211894  1.015186e-01\n",
       "q135                 0.131454  1.348945e-01\n",
       "q25                  0.127305  4.982907e-02\n",
       "q137                 0.106323  6.048333e-01\n",
       "q58                  0.105261  2.487344e-01\n",
       "q26                  0.095406  1.956756e-01\n",
       "attendance_count     0.092805  3.097618e-02\n",
       "q132                 0.063732  7.516608e-01\n",
       "q55                  0.026775  7.995295e-01\n",
       "q3                   0.014134  8.995109e-01\n",
       "q28                  0.009902  8.622489e-01\n",
       "q57                  0.001944  9.848298e-01\n",
       "q211                 0.001920  9.803918e-01\n",
       "q27                 -0.001456  9.849176e-01\n",
       "q29                 -0.007244  9.433192e-01\n",
       "q21                 -0.014983  8.621217e-01\n",
       "q23                 -0.033140  7.320063e-01\n",
       "q131                -0.038002  8.359468e-01\n",
       "q139                -0.041366  8.335151e-01\n",
       "q56                 -0.049248  6.089622e-01\n",
       "q22                 -0.072146  2.274730e-01\n",
       "q144                -0.091043  2.761526e-01\n",
       "q142                -0.126575  3.816060e-01\n",
       "q15_team_sup        -0.169645  4.328455e-01\n",
       "q210                -0.182742  4.098281e-02\n",
       "q133                -0.233661  7.832790e-02\n",
       "q138                -0.241310  2.805075e-01\n",
       "q143                -0.258296  4.357394e-04\n",
       "q53                 -0.489177  2.595475e-03\n",
       "Intercept           -9.405554  3.131363e-09"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_a=log_reg.intercept_\n",
    "lst_b=log_reg.coef_\n",
    "X_column = X_love.columns.tolist()\n",
    "\n",
    "lst=np.concatenate((lst_a, lst_b[0]))\n",
    "\n",
    "lst2=logit_pvalue(log_reg,x_train)\n",
    "\n",
    "data = pd.DataFrame(list(zip(lst, lst2)), \n",
    "              columns =['coefficient', 'p_val'],\n",
    "              index = ['Intercept'] + X_column) \n",
    "\n",
    "# from largest to smallest\n",
    "data.sort_values('coefficient', axis = 0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "executionInfo": {
     "elapsed": 16953,
     "status": "ok",
     "timestamp": 1618474185036,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "nSguXw2_tGpc",
    "outputId": "032c87aa-3954-41ac-acdc-d0577df13b37"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>p_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>q134</th>\n",
       "      <td>0.531201</td>\n",
       "      <td>7.551343e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q54</th>\n",
       "      <td>0.479985</td>\n",
       "      <td>5.800148e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q52</th>\n",
       "      <td>0.345730</td>\n",
       "      <td>1.392957e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q145</th>\n",
       "      <td>0.246509</td>\n",
       "      <td>4.042726e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q25</th>\n",
       "      <td>0.127305</td>\n",
       "      <td>4.982907e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attendance_count</th>\n",
       "      <td>0.092805</td>\n",
       "      <td>3.097618e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q210</th>\n",
       "      <td>-0.182742</td>\n",
       "      <td>4.098281e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q143</th>\n",
       "      <td>-0.258296</td>\n",
       "      <td>4.357394e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q53</th>\n",
       "      <td>-0.489177</td>\n",
       "      <td>2.595475e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>-9.405554</td>\n",
       "      <td>3.131363e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  coefficient         p_val\n",
       "q134                 0.531201  7.551343e-03\n",
       "q54                  0.479985  5.800148e-03\n",
       "q52                  0.345730  1.392957e-02\n",
       "q145                 0.246509  4.042726e-02\n",
       "q25                  0.127305  4.982907e-02\n",
       "attendance_count     0.092805  3.097618e-02\n",
       "q210                -0.182742  4.098281e-02\n",
       "q143                -0.258296  4.357394e-04\n",
       "q53                 -0.489177  2.595475e-03\n",
       "Intercept           -9.405554  3.131363e-09"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# significant\n",
    "# from largest to smallest\n",
    "data[data['p_val'] < 0.05].sort_values('coefficient', axis = 0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "executionInfo": {
     "elapsed": 16944,
     "status": "ok",
     "timestamp": 1618474185037,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "4PUkLZb0tGpc",
    "outputId": "8c82e044-62b3-4436-f916-fb7a7dd1b90e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic</th>\n",
       "      <td>0.248521</td>\n",
       "      <td>0.793814</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.794387</td>\n",
       "      <td>0.592236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Error Rate  Sensitivity  Specificity  Precision       AUC  \\\n",
       "Logistic    0.248521     0.793814     0.694444   0.777778  0.794387   \n",
       "\n",
       "          Cross-Entropy  \n",
       "Logistic       0.592236  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_score, log_loss\n",
    "\n",
    "columns=['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows=['Logistic']\n",
    "\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "y_pred = log_reg.predict(x_test) \n",
    "y_prob = log_reg.predict_proba(x_test)[:,1]\n",
    "\n",
    "confusion  = confusion_matrix(y_test, y_pred) \n",
    "results.iloc[0,0]=  1 - accuracy_score(y_test, y_pred)\n",
    "results.iloc[0,1]=  confusion[1,1]/np.sum(confusion[1,:])\n",
    "results.iloc[0,2]=  confusion[0,0]/np.sum(confusion[0,:])\n",
    "results.iloc[0,3]=  precision_score(y_test, y_pred)\n",
    "\n",
    "results.iloc[0, 4]=  roc_auc_score(y_test, y_prob)   \n",
    "results.iloc[0, 5]=  log_loss(y_test, y_prob)   \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "11tVcjzitGpd"
   },
   "outputs": [],
   "source": [
    "# y_comparison=pd.DataFrame({'y_test':y_test,\n",
    "#                           'y_pred': y_pred})\n",
    "\n",
    "# pd.DataFrame(y_comparison).to_csv('y_comparison3.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06yXRw43tGpd"
   },
   "source": [
    "## 3.3.2 Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18314,
     "status": "ok",
     "timestamp": 1618474186418,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "dFG-RneZtGpd",
    "outputId": "ca1b4cda-6964-4e01-a708-f25ae69839fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=16, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18305,
     "status": "ok",
     "timestamp": 1618474186419,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "UZBDUPA_tGpd",
    "outputId": "787949b3-a3dc-443e-a157-950c24ce7ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00439946 0.00573883 0.01340645 0.00956255 0.00955685 0.0053438\n",
      " 0.00606033 0.02421668 0.01970473 0.00722039 0.0097609  0.00704137\n",
      " 0.00901259 0.01124826 0.02067955 0.05730463 0.10097163 0.0376685\n",
      " 0.04541857 0.02785814 0.00987706 0.00918077 0.03487655 0.00997241\n",
      " 0.01056185 0.01154998 0.02529007 0.01540168 0.01706416 0.01873674\n",
      " 0.01594479 0.01332175 0.07858289 0.04205967 0.08071092 0.01145584\n",
      " 0.16323869]\n"
     ]
    }
   ],
   "source": [
    "print(rnd.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 18295,
     "status": "ok",
     "timestamp": 1618474186420,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "t-YfCQzMtGpd",
    "outputId": "971d4a97-6bf8-4b2f-82eb-10caa8035773"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>q145</th>\n",
       "      <td>0.163239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q52</th>\n",
       "      <td>0.100972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q143</th>\n",
       "      <td>0.080711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q141</th>\n",
       "      <td>0.078583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q51</th>\n",
       "      <td>0.057305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q54</th>\n",
       "      <td>0.045419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q142</th>\n",
       "      <td>0.042060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q53</th>\n",
       "      <td>0.037669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q58</th>\n",
       "      <td>0.034877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q55</th>\n",
       "      <td>0.027858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q134</th>\n",
       "      <td>0.025290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q24</th>\n",
       "      <td>0.024217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q211</th>\n",
       "      <td>0.020680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q25</th>\n",
       "      <td>0.019705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q137</th>\n",
       "      <td>0.018737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q136</th>\n",
       "      <td>0.017064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q138</th>\n",
       "      <td>0.015945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q135</th>\n",
       "      <td>0.015402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attendance_count</th>\n",
       "      <td>0.013406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q139</th>\n",
       "      <td>0.013322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q133</th>\n",
       "      <td>0.011550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q144</th>\n",
       "      <td>0.011456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q210</th>\n",
       "      <td>0.011248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q132</th>\n",
       "      <td>0.010562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q131</th>\n",
       "      <td>0.009972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q56</th>\n",
       "      <td>0.009877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q27</th>\n",
       "      <td>0.009761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q3</th>\n",
       "      <td>0.009563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q21</th>\n",
       "      <td>0.009557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q57</th>\n",
       "      <td>0.009181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q29</th>\n",
       "      <td>0.009013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q26</th>\n",
       "      <td>0.007220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q28</th>\n",
       "      <td>0.007041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q23</th>\n",
       "      <td>0.006060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q18_game_last</th>\n",
       "      <td>0.005739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q22</th>\n",
       "      <td>0.005344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q15_team_sup</th>\n",
       "      <td>0.004399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature_importance\n",
       "q145                        0.163239\n",
       "q52                         0.100972\n",
       "q143                        0.080711\n",
       "q141                        0.078583\n",
       "q51                         0.057305\n",
       "q54                         0.045419\n",
       "q142                        0.042060\n",
       "q53                         0.037669\n",
       "q58                         0.034877\n",
       "q55                         0.027858\n",
       "q134                        0.025290\n",
       "q24                         0.024217\n",
       "q211                        0.020680\n",
       "q25                         0.019705\n",
       "q137                        0.018737\n",
       "q136                        0.017064\n",
       "q138                        0.015945\n",
       "q135                        0.015402\n",
       "attendance_count            0.013406\n",
       "q139                        0.013322\n",
       "q133                        0.011550\n",
       "q144                        0.011456\n",
       "q210                        0.011248\n",
       "q132                        0.010562\n",
       "q131                        0.009972\n",
       "q56                         0.009877\n",
       "q27                         0.009761\n",
       "q3                          0.009563\n",
       "q21                         0.009557\n",
       "q57                         0.009181\n",
       "q29                         0.009013\n",
       "q26                         0.007220\n",
       "q28                         0.007041\n",
       "q23                         0.006060\n",
       "q18_game_last               0.005739\n",
       "q22                         0.005344\n",
       "q15_team_sup                0.004399"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(rnd.feature_importances_, \n",
    "              columns = ['feature_importance'],\n",
    "              index = X_column) \n",
    "\n",
    "# from largest to smallest\n",
    "data.sort_values('feature_importance', axis = 0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "executionInfo": {
     "elapsed": 18284,
     "status": "ok",
     "timestamp": 1618474186421,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "cm1T-FhvtGpe",
    "outputId": "2c20d28f-c4ac-44bb-90aa-16936d6fc05b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.814433</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.822165</td>\n",
       "      <td>0.506802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Error Rate  Sensitivity  Specificity  Precision       AUC  \\\n",
       "Random Forest    0.230769     0.814433     0.708333       0.79  0.822165   \n",
       "\n",
       "               Cross-Entropy  \n",
       "Random Forest       0.506802  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_score, log_loss\n",
    "\n",
    "columns=['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows=['Random Forest']\n",
    "\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "y_pred = rnd.predict(x_test) \n",
    "y_prob = rnd.predict_proba(x_test)[:,1]\n",
    "\n",
    "confusion  = confusion_matrix(y_test, y_pred) \n",
    "results.iloc[0,0]=  1 - accuracy_score(y_test, y_pred)\n",
    "results.iloc[0,1]=  confusion[1,1]/np.sum(confusion[1,:])\n",
    "results.iloc[0,2]=  confusion[0,0]/np.sum(confusion[0,:])\n",
    "results.iloc[0,3]=  precision_score(y_test, y_pred)\n",
    "\n",
    "results.iloc[0, 4]=  roc_auc_score(y_test, y_prob)   \n",
    "results.iloc[0, 5]=  log_loss(y_test, y_prob)   \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA6UlcuhtGpe"
   },
   "source": [
    "## 3.3.3 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "kYXxD6T4u02G"
   },
   "outputs": [],
   "source": [
    "# Installing the most recent version of skopt directly from Github\n",
    "#!pip install git+https://github.com/scikit-optimize/scikit-optimize.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95851,
     "status": "ok",
     "timestamp": 1618474422482,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "olNwzwmYtGpe",
    "outputId": "f2784c65-34ee-4afe-a926-8e9c90293d82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:14] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { min_samples_leaf, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Best parameters found: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 10, 'max_depth': 1, 'learning_rate': 0.15}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "model = XGBClassifier(reg_lambda=0)\n",
    "\n",
    "space = {\n",
    "    'n_estimators': [100, 300, 500, 1000, 2000],\n",
    "    'learning_rate': [0.005, 0.15],\n",
    "    'max_depth': [1, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10]\n",
    "}\n",
    "#np.random.seed(87)\n",
    "#xgb_search =  BayesSearchCV(model, space, cv = 5,  n_iter=32, scoring = 'neg_log_loss', n_jobs=4)\n",
    "xgb_search =  RandomizedSearchCV(model, space, cv = 5,  n_iter=32, scoring = 'neg_root_mean_squared_error', n_jobs=4)\n",
    "\n",
    "xgb_search.fit(x_train, y_train)\n",
    "\n",
    "XGB_clf = xgb_search.best_estimator_\n",
    "\n",
    "print('Best parameters found:', xgb_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95843,
     "status": "ok",
     "timestamp": 1618474422484,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "19RYCVP0tGpe",
    "outputId": "cdc713d3-54ba-41c5-e514-28acb2f5f505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.00698918 0.01369867 0.0067205  0.00547118 0.01021727\n",
      " 0.         0.03656426 0.03330077 0.0048524  0.00507566 0.00582153\n",
      " 0.01038544 0.00671492 0.         0.02300871 0.1004058  0.00917905\n",
      " 0.00517494 0.03426065 0.00792388 0.00675214 0.01537815 0.00469771\n",
      " 0.00662526 0.0062611  0.03752919 0.01011393 0.008209   0.0207555\n",
      " 0.         0.         0.16168082 0.00479553 0.08424168 0.01849309\n",
      " 0.28870204]\n"
     ]
    }
   ],
   "source": [
    "print(XGB_clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 95845,
     "status": "ok",
     "timestamp": 1618474422491,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "g0QUBwzYtGpf",
    "outputId": "5e12b14f-cc57-4163-a304-459f011d810a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>q145</th>\n",
       "      <td>0.288702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q141</th>\n",
       "      <td>0.161681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q52</th>\n",
       "      <td>0.100406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q143</th>\n",
       "      <td>0.084242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q134</th>\n",
       "      <td>0.037529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q24</th>\n",
       "      <td>0.036564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q55</th>\n",
       "      <td>0.034261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q25</th>\n",
       "      <td>0.033301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q51</th>\n",
       "      <td>0.023009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q137</th>\n",
       "      <td>0.020755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q144</th>\n",
       "      <td>0.018493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q58</th>\n",
       "      <td>0.015378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attendance_count</th>\n",
       "      <td>0.013699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q29</th>\n",
       "      <td>0.010385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q22</th>\n",
       "      <td>0.010217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q135</th>\n",
       "      <td>0.010114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q53</th>\n",
       "      <td>0.009179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q136</th>\n",
       "      <td>0.008209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q56</th>\n",
       "      <td>0.007924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q18_game_last</th>\n",
       "      <td>0.006989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q57</th>\n",
       "      <td>0.006752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q3</th>\n",
       "      <td>0.006721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q210</th>\n",
       "      <td>0.006715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q132</th>\n",
       "      <td>0.006625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q133</th>\n",
       "      <td>0.006261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q28</th>\n",
       "      <td>0.005822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q21</th>\n",
       "      <td>0.005471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q54</th>\n",
       "      <td>0.005175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q27</th>\n",
       "      <td>0.005076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q26</th>\n",
       "      <td>0.004852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q142</th>\n",
       "      <td>0.004796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q131</th>\n",
       "      <td>0.004698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q211</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q138</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q139</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q23</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q15_team_sup</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  feature_importance\n",
       "q145                        0.288702\n",
       "q141                        0.161681\n",
       "q52                         0.100406\n",
       "q143                        0.084242\n",
       "q134                        0.037529\n",
       "q24                         0.036564\n",
       "q55                         0.034261\n",
       "q25                         0.033301\n",
       "q51                         0.023009\n",
       "q137                        0.020755\n",
       "q144                        0.018493\n",
       "q58                         0.015378\n",
       "attendance_count            0.013699\n",
       "q29                         0.010385\n",
       "q22                         0.010217\n",
       "q135                        0.010114\n",
       "q53                         0.009179\n",
       "q136                        0.008209\n",
       "q56                         0.007924\n",
       "q18_game_last               0.006989\n",
       "q57                         0.006752\n",
       "q3                          0.006721\n",
       "q210                        0.006715\n",
       "q132                        0.006625\n",
       "q133                        0.006261\n",
       "q28                         0.005822\n",
       "q21                         0.005471\n",
       "q54                         0.005175\n",
       "q27                         0.005076\n",
       "q26                         0.004852\n",
       "q142                        0.004796\n",
       "q131                        0.004698\n",
       "q211                        0.000000\n",
       "q138                        0.000000\n",
       "q139                        0.000000\n",
       "q23                         0.000000\n",
       "q15_team_sup                0.000000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(XGB_clf.feature_importances_, \n",
    "              columns = ['feature_importance'],\n",
    "              index = X_column) \n",
    "\n",
    "# from largest to smallest\n",
    "data.sort_values('feature_importance', axis = 0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "executionInfo": {
     "elapsed": 95841,
     "status": "ok",
     "timestamp": 1618474422492,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "eIIKGy_btGpf",
    "outputId": "9464645d-7f2b-4c68-84d4-63a1a5352877"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XG Boost</th>\n",
       "      <td>0.248521</td>\n",
       "      <td>0.773196</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.813431</td>\n",
       "      <td>0.574401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Error Rate  Sensitivity  Specificity  Precision       AUC  \\\n",
       "XG Boost    0.248521     0.773196     0.722222   0.789474  0.813431   \n",
       "\n",
       "          Cross-Entropy  \n",
       "XG Boost       0.574401  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_score, log_loss\n",
    "\n",
    "columns=['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows=['XG Boost']\n",
    "\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "y_pred = XGB_clf.predict(x_test) \n",
    "y_prob = XGB_clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "confusion  = confusion_matrix(y_test, y_pred) \n",
    "results.iloc[0,0]=  1 - accuracy_score(y_test, y_pred)\n",
    "results.iloc[0,1]=  confusion[1,1]/np.sum(confusion[1,:])\n",
    "results.iloc[0,2]=  confusion[0,0]/np.sum(confusion[0,:])\n",
    "results.iloc[0,3]=  precision_score(y_test, y_pred)\n",
    "\n",
    "results.iloc[0, 4]=  roc_auc_score(y_test, y_prob)     \n",
    "results.iloc[0, 5]=  log_loss(y_test, y_prob)   \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfJKuQ7-tGpf"
   },
   "source": [
    "## 3.3.4 Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98211,
     "status": "ok",
     "timestamp": 1618474424868,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "enT2M-3qtGpf",
    "outputId": "b9d6fb33-48c7-4629-a58c-84f1f5efaa28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=False, with_std=True)),\n",
       "                ('linear_svc',\n",
       "                 LinearSVC(C=1, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='hinge', max_iter=1000000, multi_class='ovr',\n",
       "                           penalty='l2', random_state=None, tol=0.0001,\n",
       "                           verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "SVM_clf = Pipeline([\n",
    "         (\"scaler\", StandardScaler(with_mean=False)), \n",
    "         (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\",max_iter=1000000)), \n",
    "     ])\n",
    "\n",
    "SVM_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "executionInfo": {
     "elapsed": 98208,
     "status": "ok",
     "timestamp": 1618474424869,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "M21jo80gtGpf",
    "outputId": "d6fae34d-1b51-4d53-ba3f-106860a59394"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.278107</td>\n",
       "      <td>0.793814</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.740385</td>\n",
       "      <td>0.774914</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Error Rate  Sensitivity  Specificity  Precision       AUC  Cross-Entropy\n",
       "SVM    0.278107     0.793814        0.625   0.740385  0.774914            0.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_score, log_loss\n",
    "\n",
    "columns=['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows=['SVM']\n",
    "\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "y_pred = SVM_clf.predict(x_test) \n",
    "#y_prob = SVM_clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "confusion  = confusion_matrix(y_test, y_pred) \n",
    "results.iloc[0,0]=  1 - accuracy_score(y_test, y_pred)\n",
    "results.iloc[0,1]=  confusion[1,1]/np.sum(confusion[1,:])\n",
    "results.iloc[0,2]=  confusion[0,0]/np.sum(confusion[0,:])\n",
    "results.iloc[0,3]=  precision_score(y_test, y_pred)\n",
    "y_df = SVM_clf.decision_function(x_test)\n",
    "results.iloc[0, 4]=  roc_auc_score(y_test, y_df)   \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98204,
     "status": "ok",
     "timestamp": 1618474424870,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "BPaPrUYPtGpg",
    "outputId": "3f3eb592-5565-4761-f069-51f364b441f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=10000000, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm = svm.SVC(kernel='linear',max_iter=10000000)\n",
    "svm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 98200,
     "status": "ok",
     "timestamp": 1618474424871,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "jC_lwkeItGpg",
    "outputId": "42707dbb-e971-43f2-de74-8963c37ff052"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>q54</td>\n",
       "      <td>0.435654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q18_game_last</td>\n",
       "      <td>0.413308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>q53</td>\n",
       "      <td>-0.399337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>q134</td>\n",
       "      <td>0.333796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>q136</td>\n",
       "      <td>0.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>q52</td>\n",
       "      <td>0.263143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>q133</td>\n",
       "      <td>-0.238776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>q143</td>\n",
       "      <td>-0.212575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q15_team_sup</td>\n",
       "      <td>-0.201182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>q145</td>\n",
       "      <td>0.199071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>q24</td>\n",
       "      <td>0.155431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>q141</td>\n",
       "      <td>0.153551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>q51</td>\n",
       "      <td>0.138677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>q138</td>\n",
       "      <td>-0.119194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>q135</td>\n",
       "      <td>0.116908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>q210</td>\n",
       "      <td>-0.108006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>q25</td>\n",
       "      <td>0.080499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>q142</td>\n",
       "      <td>-0.079213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>q132</td>\n",
       "      <td>0.073650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>q144</td>\n",
       "      <td>-0.060238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>q26</td>\n",
       "      <td>0.052267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attendance_count</td>\n",
       "      <td>0.050763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q3</td>\n",
       "      <td>0.049818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>q131</td>\n",
       "      <td>-0.047285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>q58</td>\n",
       "      <td>0.045584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>q55</td>\n",
       "      <td>0.044580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>q22</td>\n",
       "      <td>-0.044126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>q28</td>\n",
       "      <td>0.042342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>q29</td>\n",
       "      <td>-0.032398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>q211</td>\n",
       "      <td>-0.026994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>q57</td>\n",
       "      <td>-0.020434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>q27</td>\n",
       "      <td>-0.013077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>q139</td>\n",
       "      <td>0.012204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>q23</td>\n",
       "      <td>-0.011567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q21</td>\n",
       "      <td>-0.009875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>q137</td>\n",
       "      <td>0.004917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>q56</td>\n",
       "      <td>-0.004876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   X  feature_importance\n",
       "18               q54            0.435654\n",
       "1      q18_game_last            0.413308\n",
       "17               q53           -0.399337\n",
       "26              q134            0.333796\n",
       "28              q136            0.321000\n",
       "16               q52            0.263143\n",
       "25              q133           -0.238776\n",
       "34              q143           -0.212575\n",
       "0       q15_team_sup           -0.201182\n",
       "36              q145            0.199071\n",
       "7                q24            0.155431\n",
       "32              q141            0.153551\n",
       "15               q51            0.138677\n",
       "30              q138           -0.119194\n",
       "27              q135            0.116908\n",
       "13              q210           -0.108006\n",
       "8                q25            0.080499\n",
       "33              q142           -0.079213\n",
       "24              q132            0.073650\n",
       "35              q144           -0.060238\n",
       "9                q26            0.052267\n",
       "2   attendance_count            0.050763\n",
       "3                 q3            0.049818\n",
       "23              q131           -0.047285\n",
       "22               q58            0.045584\n",
       "19               q55            0.044580\n",
       "5                q22           -0.044126\n",
       "11               q28            0.042342\n",
       "12               q29           -0.032398\n",
       "14              q211           -0.026994\n",
       "21               q57           -0.020434\n",
       "10               q27           -0.013077\n",
       "31              q139            0.012204\n",
       "6                q23           -0.011567\n",
       "4                q21           -0.009875\n",
       "29              q137            0.004917\n",
       "20               q56           -0.004876"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = svm.coef_\n",
    "\n",
    "data = pd.DataFrame({\"X\": X_column, \"feature_importance\": feature_importance[0]})\n",
    "\n",
    "# from largest to smallest - absolute value\n",
    "data.sort_values('feature_importance', axis = 0, ascending = False, key = abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "executionInfo": {
     "elapsed": 98197,
     "status": "ok",
     "timestamp": 1618474424872,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "n2GXcIEQtGpg",
    "outputId": "57fb6ede-10b0-47c8-fbe5-cf2b44ea6d38"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.284024</td>\n",
       "      <td>0.793814</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.793958</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Error Rate  Sensitivity  Specificity  Precision       AUC  Cross-Entropy\n",
       "SVM    0.284024     0.793814     0.611111   0.733333  0.793958            0.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_score, log_loss\n",
    "\n",
    "columns=['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows=['SVM']\n",
    "\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "y_pred = svm.predict(x_test) \n",
    "#y_prob = SVM_clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "confusion  = confusion_matrix(y_test, y_pred) \n",
    "results.iloc[0,0]=  1 - accuracy_score(y_test, y_pred)\n",
    "results.iloc[0,1]=  confusion[1,1]/np.sum(confusion[1,:])\n",
    "results.iloc[0,2]=  confusion[0,0]/np.sum(confusion[0,:])\n",
    "results.iloc[0,3]=  precision_score(y_test, y_pred)\n",
    "y_df = svm.decision_function(x_test)\n",
    "results.iloc[0, 4]=  roc_auc_score(y_test, y_df)   \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "UffgJ7OHc8hk",
    "outputId": "61a9cc88-33cf-4862-9bdb-e51df851b46a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logit Model</th>\n",
       "      <td>0.248521</td>\n",
       "      <td>0.793814</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.794387</td>\n",
       "      <td>0.592236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forests</th>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.814433</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.822165</td>\n",
       "      <td>0.506802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.248521</td>\n",
       "      <td>0.773196</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.813431</td>\n",
       "      <td>0.574401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.284024</td>\n",
       "      <td>0.793814</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.793958</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Error Rate  Sensitivity  Specificity  Precision       AUC  \\\n",
       "Logit Model          0.248521     0.793814     0.694444   0.777778  0.794387   \n",
       "Random Forests       0.230769     0.814433     0.708333   0.790000  0.822165   \n",
       "Gradient Boosting    0.248521     0.773196     0.722222   0.789474  0.813431   \n",
       "SVM                  0.284024     0.793814     0.611111   0.733333  0.793958   \n",
       "\n",
       "                   Cross-Entropy  \n",
       "Logit Model             0.592236  \n",
       "Random Forests          0.506802  \n",
       "Gradient Boosting       0.574401  \n",
       "SVM                     0.000000  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, precision_score, log_loss\n",
    "\n",
    "columns = ['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows = ['Logit Model', 'Random Forests', 'Gradient Boosting', 'SVM']\n",
    "results = pd.DataFrame(0.0, columns = columns, index = rows) \n",
    "\n",
    "methods = [log_reg, rnd, XGB_clf, svm]\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    if method != svm:\n",
    "        y_pred = method.predict(x_test) \n",
    "        y_prob = method.predict_proba(x_test)[:,1]\n",
    "        confusion = confusion_matrix(y_test, y_pred) \n",
    "        results.iloc[i,0] = 1 - accuracy_score(y_test, y_pred)\n",
    "        results.iloc[i,1] = confusion[1,1]/np.sum(confusion[1,:])\n",
    "        results.iloc[i,2] = confusion[0,0]/np.sum(confusion[0,:])\n",
    "        results.iloc[i,3] = precision_score(y_test, y_pred)\n",
    "        results.iloc[i,4] = roc_auc_score(y_test, y_prob)   \n",
    "        results.iloc[i,5] = log_loss(y_test, y_prob)\n",
    "    else:\n",
    "        y_pred = method.predict(x_test) \n",
    "        confusion = confusion_matrix(y_test, y_pred) \n",
    "        results.iloc[i,0] = 1 - accuracy_score(y_test, y_pred)\n",
    "        results.iloc[i,1] = confusion[1,1]/np.sum(confusion[1,:])\n",
    "        results.iloc[i,2] = confusion[0,0]/np.sum(confusion[0,:])\n",
    "        results.iloc[i,3] = precision_score(y_test, y_pred)\n",
    "        y_df = svm.decision_function(x_test)\n",
    "        results.iloc[i,4] = roc_auc_score(y_test, y_df) \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14jQcnyJEotf"
   },
   "source": [
    "# 3.4 What can Swans organization do to achieve a trustworthy manner towards its fans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "executionInfo": {
     "elapsed": 98192,
     "status": "ok",
     "timestamp": 1618474424872,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "h-1lzuBUEoti",
    "outputId": "84ed6187-d975-41bd-b0bd-ae58b14be6b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q911</th>\n",
       "      <th>q912</th>\n",
       "      <th>q921</th>\n",
       "      <th>q922</th>\n",
       "      <th>q923</th>\n",
       "      <th>q924</th>\n",
       "      <th>q931</th>\n",
       "      <th>q932</th>\n",
       "      <th>q933</th>\n",
       "      <th>q934</th>\n",
       "      <th>q941</th>\n",
       "      <th>q942</th>\n",
       "      <th>q943</th>\n",
       "      <th>q951</th>\n",
       "      <th>q952</th>\n",
       "      <th>q953</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>1.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>674 rows  16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     q911  q912  q921  q922  q923  q924  q931  q932  q933  q934  q941  q942  \\\n",
       "0     0.6   0.5  0.90  1.05  0.75  0.90  0.75  1.05   0.9  0.75  1.80  1.80   \n",
       "1     1.0   1.2  1.00  1.00  1.00  1.20  1.00  1.00   0.8  1.00  1.20  1.20   \n",
       "2     1.2   0.8  1.75  1.25  1.25  1.25  1.00  1.20   0.8  1.00  1.20  1.40   \n",
       "3     1.2   1.2  0.40  0.40  0.40  0.40  0.40  0.40   0.4  0.40  0.80  0.80   \n",
       "4     1.5   1.5  0.10  0.05  0.05  0.15  0.40  1.00   1.2  1.00  2.40  2.40   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "669   0.8   1.0  1.00  1.20  0.80  1.40  2.40  2.00   2.8  1.20  1.40  1.40   \n",
       "670   1.2   1.0  1.20  1.20  1.00  1.00  1.00  1.00   0.8  0.80  1.20  1.20   \n",
       "671   1.2   1.0  1.50  1.80  1.80  1.80  0.80  1.20   1.2  1.20  0.75  0.75   \n",
       "672   0.3   0.3  2.40  2.80  2.40  2.40  0.30  0.30   0.3  0.30  1.20  1.20   \n",
       "673   0.5   0.4  1.05  0.75  0.90  0.90  0.60  0.60   0.6  0.50  3.00  3.00   \n",
       "\n",
       "     q943  q951  q952  q953  \n",
       "0    1.80  2.10  2.10  2.10  \n",
       "1    1.20  1.20  1.20  1.20  \n",
       "2    1.00  1.05  0.75  0.90  \n",
       "3    0.80  1.20  1.20  1.20  \n",
       "4    0.40  0.70  0.70  0.70  \n",
       "..    ...   ...   ...   ...  \n",
       "669  1.20  0.00  0.00  0.00  \n",
       "670  1.20  0.80  1.00  0.60  \n",
       "671  0.75  0.75  0.75  0.75  \n",
       "672  1.40  1.80  1.80  1.80  \n",
       "673  3.50  0.90  0.75  1.05  \n",
       "\n",
       "[674 rows x 16 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_Q9=all_df[[ #'q711','q712','q721','q722','q723','q731','q732','q741','q742','q751',\n",
    "             'q911','q912','q921','q922','q923','q924','q931','q932','q933','q934','q941','q942','q943','q951','q952','q953']]\n",
    "\n",
    "all_df['trustworthy']=(all_df['q134']>6).astype(int)\n",
    "y_Q9=all_df['trustworthy']\n",
    "X_Q9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vh9sdXq_Eoti"
   },
   "source": [
    "## 3.4.1 Logit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "x3KvcbT7Eotj"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_Q9, y_Q9, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98186,
     "status": "ok",
     "timestamp": 1618474424875,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "Y0L0Fdp1Eotj",
    "outputId": "c42858a7-f7f4-44d7-d86f-f38e47ccc6bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(max_iter=10000)\n",
    "log_reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98182,
     "status": "ok",
     "timestamp": 1618474424876,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "AghtNThhEotj",
    "outputId": "31ad52e7-0c33-413e-ffa5-409123be7ca5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.62417168]\n",
      "[[ 1.22469486  0.25068808  0.43632776 -0.01977411  0.2768983   1.00919939\n",
      "   0.64742106  1.16666851  0.24450789 -0.58579353  0.75187492  0.1308931\n",
      "   0.72205373  0.68138968  0.0505029   0.64731943]]\n"
     ]
    }
   ],
   "source": [
    "print(log_reg.intercept_)\n",
    "print(log_reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98177,
     "status": "ok",
     "timestamp": 1618474424876,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "2Tbc2BDwEotk",
    "outputId": "b8bf0e19-a64f-4b79-8c4c-2696c6362936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.01264962 0.59146203 0.5029796  0.96900539 0.56076575\n",
      " 0.07030534 0.42749435 0.20063368 0.79951412 0.39413157 0.46038672\n",
      " 0.90216593 0.18756228 0.37244746 0.94920368 0.10622658]\n"
     ]
    }
   ],
   "source": [
    "# Get P_values\n",
    "from scipy.stats import norm\n",
    "\n",
    "def logit_pvalue(model, x):\n",
    "    p = model.predict_proba(x)\n",
    "    n = len(p)\n",
    "    m = len(model.coef_[0]) + 1\n",
    "    coefs = np.concatenate([model.intercept_, model.coef_[0]])\n",
    "    x_full = np.matrix(np.insert(np.array(x), 0, 1, axis = 1))\n",
    "    ans = np.zeros((m, m))\n",
    "    for i in range(n):\n",
    "        ans = ans + np.dot(np.transpose(x_full[i, :]), x_full[i, :]) * p[i,1] * p[i, 0]\n",
    "    vcov = np.linalg.inv(np.matrix(ans))\n",
    "    se = np.sqrt(np.diag(vcov))\n",
    "    t =  coefs/se  \n",
    "    p = (1 - norm.cdf(abs(t))) * 2\n",
    "    return p\n",
    "\n",
    "pvalue = logit_pvalue(log_reg, x_train)\n",
    "print(pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 877
    },
    "executionInfo": {
     "elapsed": 98174,
     "status": "ok",
     "timestamp": 1618474424877,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "D9V3GGTJ_yL4",
    "outputId": "4c2f5f28-d3e6-43b1-f755-486e92fc6d57"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>p_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>q911</th>\n",
       "      <td>1.224695</td>\n",
       "      <td>0.012650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q932</th>\n",
       "      <td>1.166669</td>\n",
       "      <td>0.200634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q924</th>\n",
       "      <td>1.009199</td>\n",
       "      <td>0.070305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q941</th>\n",
       "      <td>0.751875</td>\n",
       "      <td>0.460387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q943</th>\n",
       "      <td>0.722054</td>\n",
       "      <td>0.187562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q951</th>\n",
       "      <td>0.681390</td>\n",
       "      <td>0.372447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q931</th>\n",
       "      <td>0.647421</td>\n",
       "      <td>0.427494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q953</th>\n",
       "      <td>0.647319</td>\n",
       "      <td>0.106227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q921</th>\n",
       "      <td>0.436328</td>\n",
       "      <td>0.502980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q923</th>\n",
       "      <td>0.276898</td>\n",
       "      <td>0.560766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q912</th>\n",
       "      <td>0.250688</td>\n",
       "      <td>0.591462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q933</th>\n",
       "      <td>0.244508</td>\n",
       "      <td>0.799514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q942</th>\n",
       "      <td>0.130893</td>\n",
       "      <td>0.902166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q952</th>\n",
       "      <td>0.050503</td>\n",
       "      <td>0.949204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q922</th>\n",
       "      <td>-0.019774</td>\n",
       "      <td>0.969005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q934</th>\n",
       "      <td>-0.585794</td>\n",
       "      <td>0.394132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>-9.624172</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           coefficient     p_val\n",
       "q911          1.224695  0.012650\n",
       "q932          1.166669  0.200634\n",
       "q924          1.009199  0.070305\n",
       "q941          0.751875  0.460387\n",
       "q943          0.722054  0.187562\n",
       "q951          0.681390  0.372447\n",
       "q931          0.647421  0.427494\n",
       "q953          0.647319  0.106227\n",
       "q921          0.436328  0.502980\n",
       "q923          0.276898  0.560766\n",
       "q912          0.250688  0.591462\n",
       "q933          0.244508  0.799514\n",
       "q942          0.130893  0.902166\n",
       "q952          0.050503  0.949204\n",
       "q922         -0.019774  0.969005\n",
       "q934         -0.585794  0.394132\n",
       "Intercept    -9.624172  0.000000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_a=log_reg.intercept_\n",
    "lst_b=log_reg.coef_\n",
    "X_column = X_Q9.columns.tolist()\n",
    "\n",
    "lst=np.concatenate((lst_a, lst_b[0]))\n",
    "\n",
    "lst2=logit_pvalue(log_reg,x_train)\n",
    "\n",
    "data = pd.DataFrame(list(zip(lst, lst2)), \n",
    "              columns =['coefficient', 'p_val'],\n",
    "              index = ['Intercept'] + X_column) \n",
    "\n",
    "# from largest to smallest\n",
    "data.sort_values('coefficient', axis = 0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 98169,
     "status": "ok",
     "timestamp": 1618474424877,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "PgAyq5k5AIQk",
    "outputId": "95808137-3609-4e90-b774-a3bdd8099e75"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficient</th>\n",
       "      <th>p_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Intercept</th>\n",
       "      <td>-9.624172</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q911</th>\n",
       "      <td>1.224695</td>\n",
       "      <td>0.01265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           coefficient    p_val\n",
       "Intercept    -9.624172  0.00000\n",
       "q911          1.224695  0.01265"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# significant\n",
    "data[data['p_val'] < 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "executionInfo": {
     "elapsed": 98165,
     "status": "ok",
     "timestamp": 1618474424878,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "WajK34AQEotk",
    "outputId": "e909ee91-ad7c-4544-995e-4692fd49d30b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic</th>\n",
       "      <td>0.171598</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.905512</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.846457</td>\n",
       "      <td>0.419866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Error Rate  Sensitivity  Specificity  Precision       AUC  \\\n",
       "Logistic    0.171598     0.595238     0.905512   0.675676  0.846457   \n",
       "\n",
       "          Cross-Entropy  \n",
       "Logistic       0.419866  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_score, log_loss\n",
    "\n",
    "columns=['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows=['Logistic']\n",
    "\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "y_pred = log_reg.predict(x_test) \n",
    "y_prob = log_reg.predict_proba(x_test)[:,1]\n",
    "\n",
    "confusion  = confusion_matrix(y_test, y_pred) \n",
    "results.iloc[0,0]=  1 - accuracy_score(y_test, y_pred)\n",
    "results.iloc[0,1]=  confusion[1,1]/np.sum(confusion[1,:])\n",
    "results.iloc[0,2]=  confusion[0,0]/np.sum(confusion[0,:])\n",
    "results.iloc[0,3]=  precision_score(y_test, y_pred)\n",
    "\n",
    "results.iloc[0, 4]=  roc_auc_score(y_test, y_prob)   \n",
    "results.iloc[0, 5]=  log_loss(y_test, y_prob)   \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "iq_0IS4lEotl"
   },
   "outputs": [],
   "source": [
    "# y_comparison=pd.DataFrame({'y_test':y_test,\n",
    "#                           'y_pred': y_pred})\n",
    "\n",
    "# pd.DataFrame(y_comparison).to_csv('y_comparison2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iCvjSyEEotl"
   },
   "source": [
    "## 3.4.2 Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 99877,
     "status": "ok",
     "timestamp": 1618474426599,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "U5379C0kEotm",
    "outputId": "5afde43e-3d88-47a5-bd5b-67c78bf8f41d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=16, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 99873,
     "status": "ok",
     "timestamp": 1618474426600,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "n1fsHT8GAhyt",
    "outputId": "c4933442-6df5-4aa6-92e8-9a0c29273c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04442265 0.04147251 0.05388106 0.04723912 0.05866825 0.06033451\n",
      " 0.08198181 0.0762391  0.05211774 0.05888245 0.08701702 0.07932473\n",
      " 0.11885364 0.03970652 0.04958207 0.05027682]\n"
     ]
    }
   ],
   "source": [
    "print(rnd.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846
    },
    "executionInfo": {
     "elapsed": 99869,
     "status": "ok",
     "timestamp": 1618474426601,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "ZTxvHQ5kAlSI",
    "outputId": "f48b8e3e-de65-40cf-a025-5a6fe6a238b5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>q943</th>\n",
       "      <td>0.118854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q941</th>\n",
       "      <td>0.087017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q931</th>\n",
       "      <td>0.081982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q942</th>\n",
       "      <td>0.079325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q932</th>\n",
       "      <td>0.076239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q924</th>\n",
       "      <td>0.060335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q934</th>\n",
       "      <td>0.058882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q923</th>\n",
       "      <td>0.058668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q921</th>\n",
       "      <td>0.053881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q933</th>\n",
       "      <td>0.052118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q953</th>\n",
       "      <td>0.050277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q952</th>\n",
       "      <td>0.049582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q922</th>\n",
       "      <td>0.047239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q911</th>\n",
       "      <td>0.044423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q912</th>\n",
       "      <td>0.041473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q951</th>\n",
       "      <td>0.039707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_importance\n",
       "q943            0.118854\n",
       "q941            0.087017\n",
       "q931            0.081982\n",
       "q942            0.079325\n",
       "q932            0.076239\n",
       "q924            0.060335\n",
       "q934            0.058882\n",
       "q923            0.058668\n",
       "q921            0.053881\n",
       "q933            0.052118\n",
       "q953            0.050277\n",
       "q952            0.049582\n",
       "q922            0.047239\n",
       "q911            0.044423\n",
       "q912            0.041473\n",
       "q951            0.039707"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(rnd.feature_importances_, \n",
    "              columns = ['feature_importance'],\n",
    "              index = X_column) \n",
    "\n",
    "# from largest to smallest\n",
    "data.sort_values('feature_importance', axis = 0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "executionInfo": {
     "elapsed": 100266,
     "status": "ok",
     "timestamp": 1618474427002,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "hS21SOWnEotn",
    "outputId": "662dba79-fd8b-4c8c-937a-e8dc6f962fe8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.142012</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.96063</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.839708</td>\n",
       "      <td>0.41994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Error Rate  Sensitivity  Specificity  Precision       AUC  \\\n",
       "Random Forest    0.142012     0.547619      0.96063   0.821429  0.839708   \n",
       "\n",
       "               Cross-Entropy  \n",
       "Random Forest        0.41994  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_score, log_loss\n",
    "\n",
    "columns=['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows=['Random Forest']\n",
    "\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "y_pred = rnd.predict(x_test) \n",
    "y_prob = rnd.predict_proba(x_test)[:,1]\n",
    "\n",
    "confusion  = confusion_matrix(y_test, y_pred) \n",
    "results.iloc[0,0]=  1 - accuracy_score(y_test, y_pred)\n",
    "results.iloc[0,1]=  confusion[1,1]/np.sum(confusion[1,:])\n",
    "results.iloc[0,2]=  confusion[0,0]/np.sum(confusion[0,:])\n",
    "results.iloc[0,3]=  precision_score(y_test, y_pred)\n",
    "\n",
    "results.iloc[0, 4]=  roc_auc_score(y_test, y_prob)   \n",
    "results.iloc[0, 5]=  log_loss(y_test, y_prob)   \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZdFKvTREoto"
   },
   "source": [
    "## 3.4.3 Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 184484,
     "status": "ok",
     "timestamp": 1618474511225,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "dKtVOoQ1Eotp",
    "outputId": "f45a2394-7719-4625-a429-3421b98fe0c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:39] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.1.0\\src\\learner.cc:480: \n",
      "Parameters: { min_samples_leaf, min_samples_split } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "Best parameters found: {'n_estimators': 2000, 'min_samples_split': 20, 'min_samples_leaf': 1, 'max_depth': 5, 'learning_rate': 0.005}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(reg_lambda=0)\n",
    "\n",
    "space = {\n",
    "    'n_estimators': [100, 300, 500, 1000, 2000],\n",
    "    'learning_rate': [0.005, 0.15],\n",
    "    'max_depth': [1, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10]\n",
    "}\n",
    "#np.random.seed(87)\n",
    "#xgb_search =  BayesSearchCV(model, space, cv = 5,  n_iter=32, scoring = 'neg_log_loss', n_jobs=4)\n",
    "xgb_search =  RandomizedSearchCV(model, space, cv = 5,  n_iter=32, scoring = 'neg_root_mean_squared_error', n_jobs=4)\n",
    "\n",
    "xgb_search.fit(x_train, y_train)\n",
    "\n",
    "XGB_clf = xgb_search.best_estimator_\n",
    "\n",
    "print('Best parameters found:', xgb_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 184480,
     "status": "ok",
     "timestamp": 1618474511226,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "ClI38xdZEotr",
    "outputId": "2be4a7b4-19d2-4c11-b609-61308771c071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05279732 0.05018838 0.07284068 0.04434277 0.05315758 0.06341276\n",
      " 0.10084812 0.04988442 0.04466386 0.0608737  0.08222464 0.05754057\n",
      " 0.09956696 0.0440809  0.07180896 0.05176838]\n"
     ]
    }
   ],
   "source": [
    "print(XGB_clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846
    },
    "executionInfo": {
     "elapsed": 185168,
     "status": "ok",
     "timestamp": 1618474511918,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "3MRb2AHYA9H5",
    "outputId": "0d019c51-b835-4124-cdc5-56486361c1dd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>q931</th>\n",
       "      <td>0.100848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q943</th>\n",
       "      <td>0.099567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q941</th>\n",
       "      <td>0.082225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q921</th>\n",
       "      <td>0.072841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q952</th>\n",
       "      <td>0.071809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q924</th>\n",
       "      <td>0.063413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q934</th>\n",
       "      <td>0.060874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q942</th>\n",
       "      <td>0.057541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q923</th>\n",
       "      <td>0.053158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q911</th>\n",
       "      <td>0.052797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q953</th>\n",
       "      <td>0.051768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q912</th>\n",
       "      <td>0.050188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q932</th>\n",
       "      <td>0.049884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q933</th>\n",
       "      <td>0.044664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q922</th>\n",
       "      <td>0.044343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q951</th>\n",
       "      <td>0.044081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature_importance\n",
       "q931            0.100848\n",
       "q943            0.099567\n",
       "q941            0.082225\n",
       "q921            0.072841\n",
       "q952            0.071809\n",
       "q924            0.063413\n",
       "q934            0.060874\n",
       "q942            0.057541\n",
       "q923            0.053158\n",
       "q911            0.052797\n",
       "q953            0.051768\n",
       "q912            0.050188\n",
       "q932            0.049884\n",
       "q933            0.044664\n",
       "q922            0.044343\n",
       "q951            0.044081"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(XGB_clf.feature_importances_, \n",
    "              columns = ['feature_importance'],\n",
    "              index = X_column) \n",
    "\n",
    "# from largest to smallest\n",
    "data.sort_values('feature_importance', axis = 0, ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "executionInfo": {
     "elapsed": 185164,
     "status": "ok",
     "timestamp": 1618474511919,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "nhswr_7AEotq",
    "outputId": "d80a2cc2-705f-4474-b81f-eea338cfe369"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XG Boost</th>\n",
       "      <td>0.207101</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.824709</td>\n",
       "      <td>0.440853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Error Rate  Sensitivity  Specificity  Precision       AUC  \\\n",
       "XG Boost    0.207101     0.547619     0.874016   0.589744  0.824709   \n",
       "\n",
       "          Cross-Entropy  \n",
       "XG Boost       0.440853  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_score, log_loss\n",
    "\n",
    "columns=['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows=['XG Boost']\n",
    "\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "y_pred = XGB_clf.predict(x_test) \n",
    "y_prob = XGB_clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "confusion  = confusion_matrix(y_test, y_pred) \n",
    "results.iloc[0,0]=  1 - accuracy_score(y_test, y_pred)\n",
    "results.iloc[0,1]=  confusion[1,1]/np.sum(confusion[1,:])\n",
    "results.iloc[0,2]=  confusion[0,0]/np.sum(confusion[0,:])\n",
    "results.iloc[0,3]=  precision_score(y_test, y_pred)\n",
    "\n",
    "results.iloc[0, 4]=  roc_auc_score(y_test, y_prob)     \n",
    "results.iloc[0, 5]=  log_loss(y_test, y_prob)   \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wz_I6l4REots"
   },
   "source": [
    "## 3.4.4 Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 185159,
     "status": "ok",
     "timestamp": 1618474511919,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "3JHL6InCEott",
    "outputId": "709d16c3-6968-4676-a338-51756f66c383"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=False, with_std=True)),\n",
       "                ('linear_svc',\n",
       "                 LinearSVC(C=1, class_weight=None, dual=True,\n",
       "                           fit_intercept=True, intercept_scaling=1,\n",
       "                           loss='hinge', max_iter=1000000, multi_class='ovr',\n",
       "                           penalty='l2', random_state=None, tol=0.0001,\n",
       "                           verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_clf = Pipeline([\n",
    "         (\"scaler\", StandardScaler(with_mean=False)), \n",
    "         (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\",max_iter=1000000)), \n",
    "     ])\n",
    "\n",
    "SVM_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "executionInfo": {
     "elapsed": 185156,
     "status": "ok",
     "timestamp": 1618474511921,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "dAQs1lgaEotu",
    "outputId": "a6daa41a-4d6c-44e1-aead-d9399f3f9d42"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.171598</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.984252</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.836895</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Error Rate  Sensitivity  Specificity  Precision       AUC  Cross-Entropy\n",
       "SVM    0.171598     0.357143     0.984252   0.882353  0.836895            0.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_score, log_loss\n",
    "\n",
    "columns=['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows=['SVM']\n",
    "\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "y_pred = SVM_clf.predict(x_test) \n",
    "#y_prob = SVM_clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "confusion  = confusion_matrix(y_test, y_pred) \n",
    "results.iloc[0,0]=  1 - accuracy_score(y_test, y_pred)\n",
    "results.iloc[0,1]=  confusion[1,1]/np.sum(confusion[1,:])\n",
    "results.iloc[0,2]=  confusion[0,0]/np.sum(confusion[0,:])\n",
    "results.iloc[0,3]=  precision_score(y_test, y_pred)\n",
    "y_df = SVM_clf.decision_function(x_test)\n",
    "results.iloc[0, 4]=  roc_auc_score(y_test, y_df)   \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 185153,
     "status": "ok",
     "timestamp": 1618474511922,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "6D43EYvDEotv",
    "outputId": "9e6a886d-d60b-4305-b456-6bba72dbf4b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=10000000, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm = svm.SVC(kernel='linear',max_iter=10000000)\n",
    "svm.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846
    },
    "executionInfo": {
     "elapsed": 185149,
     "status": "ok",
     "timestamp": 1618474511923,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "b1oeefO0BBiT",
    "outputId": "0fb975ef-be21-4e7c-e42e-404f11472fae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q911</td>\n",
       "      <td>1.397417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>q932</td>\n",
       "      <td>1.103141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>q924</td>\n",
       "      <td>1.057163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>q943</td>\n",
       "      <td>0.818916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>q951</td>\n",
       "      <td>0.777526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>q941</td>\n",
       "      <td>0.620646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>q931</td>\n",
       "      <td>0.569754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>q953</td>\n",
       "      <td>0.476829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>q934</td>\n",
       "      <td>-0.433093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q921</td>\n",
       "      <td>0.243936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>q933</td>\n",
       "      <td>0.180288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q923</td>\n",
       "      <td>0.160917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q922</td>\n",
       "      <td>0.078019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>q952</td>\n",
       "      <td>0.062605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q912</td>\n",
       "      <td>0.049589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>q942</td>\n",
       "      <td>0.003380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       X  feature_importance\n",
       "0   q911            1.397417\n",
       "7   q932            1.103141\n",
       "5   q924            1.057163\n",
       "12  q943            0.818916\n",
       "13  q951            0.777526\n",
       "10  q941            0.620646\n",
       "6   q931            0.569754\n",
       "15  q953            0.476829\n",
       "9   q934           -0.433093\n",
       "2   q921            0.243936\n",
       "8   q933            0.180288\n",
       "4   q923            0.160917\n",
       "3   q922            0.078019\n",
       "14  q952            0.062605\n",
       "1   q912            0.049589\n",
       "11  q942            0.003380"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance = svm.coef_\n",
    "\n",
    "data = pd.DataFrame({\"X\": X_column, \"feature_importance\": feature_importance[0]})\n",
    "\n",
    "# from largest to smallest - absolute value\n",
    "data.sort_values('feature_importance', axis = 0, ascending = False, key = abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "executionInfo": {
     "elapsed": 185146,
     "status": "ok",
     "timestamp": 1618474511925,
     "user": {
      "displayName": "Selina Yang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjT3HwEF_guMLseEd9y9DckP5QSvTDH384LXiQd=s64",
      "userId": "09018571664119049451"
     },
     "user_tz": -600
    },
    "id": "g7PCAoRSREe0",
    "outputId": "27142db2-2039-4dc1-8dcd-4056c5acc8eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.177515</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.847394</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Error Rate  Sensitivity  Specificity  Precision       AUC  Cross-Entropy\n",
       "SVM    0.177515     0.666667     0.874016   0.636364  0.847394            0.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_score, log_loss\n",
    "\n",
    "columns=['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows=['SVM']\n",
    "\n",
    "results=pd.DataFrame(0.0, columns=columns, index=rows) \n",
    "\n",
    "y_pred = svm.predict(x_test) \n",
    "#y_prob = SVM_clf.predict_proba(x_test)[:,1]\n",
    "\n",
    "confusion  = confusion_matrix(y_test, y_pred) \n",
    "results.iloc[0,0]=  1 - accuracy_score(y_test, y_pred)\n",
    "results.iloc[0,1]=  confusion[1,1]/np.sum(confusion[1,:])\n",
    "results.iloc[0,2]=  confusion[0,0]/np.sum(confusion[0,:])\n",
    "results.iloc[0,3]=  precision_score(y_test, y_pred)\n",
    "y_df = svm.decision_function(x_test)\n",
    "results.iloc[0, 4]=  roc_auc_score(y_test, y_df)   \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "MwJzCYq3c8hq",
    "outputId": "5703d4a6-c896-42f5-8251-7bf51c46fbe2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Error Rate</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Cross-Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logit Model</th>\n",
       "      <td>0.171598</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.905512</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.846457</td>\n",
       "      <td>0.419866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forests</th>\n",
       "      <td>0.142012</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.960630</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.839708</td>\n",
       "      <td>0.419940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.207101</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.824709</td>\n",
       "      <td>0.440853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.177515</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.874016</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.847394</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Error Rate  Sensitivity  Specificity  Precision       AUC  \\\n",
       "Logit Model          0.171598     0.595238     0.905512   0.675676  0.846457   \n",
       "Random Forests       0.142012     0.547619     0.960630   0.821429  0.839708   \n",
       "Gradient Boosting    0.207101     0.547619     0.874016   0.589744  0.824709   \n",
       "SVM                  0.177515     0.666667     0.874016   0.636364  0.847394   \n",
       "\n",
       "                   Cross-Entropy  \n",
       "Logit Model             0.419866  \n",
       "Random Forests          0.419940  \n",
       "Gradient Boosting       0.440853  \n",
       "SVM                     0.000000  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, precision_score, log_loss\n",
    "\n",
    "columns = ['Error Rate', 'Sensitivity', 'Specificity', 'Precision', 'AUC', 'Cross-Entropy']\n",
    "rows = ['Logit Model', 'Random Forests', 'Gradient Boosting', 'SVM']\n",
    "results = pd.DataFrame(0.0, columns = columns, index = rows) \n",
    "\n",
    "methods = [log_reg, rnd, XGB_clf, svm]\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    if method != svm:\n",
    "        y_pred = method.predict(x_test) \n",
    "        y_prob = method.predict_proba(x_test)[:,1]\n",
    "        confusion = confusion_matrix(y_test, y_pred) \n",
    "        results.iloc[i,0] = 1 - accuracy_score(y_test, y_pred)\n",
    "        results.iloc[i,1] = confusion[1,1]/np.sum(confusion[1,:])\n",
    "        results.iloc[i,2] = confusion[0,0]/np.sum(confusion[0,:])\n",
    "        results.iloc[i,3] = precision_score(y_test, y_pred)\n",
    "        results.iloc[i,4] = roc_auc_score(y_test, y_prob)   \n",
    "        results.iloc[i,5] = log_loss(y_test, y_prob)\n",
    "    else:\n",
    "        y_pred = method.predict(x_test) \n",
    "        confusion = confusion_matrix(y_test, y_pred) \n",
    "        results.iloc[i,0] = 1 - accuracy_score(y_test, y_pred)\n",
    "        results.iloc[i,1] = confusion[1,1]/np.sum(confusion[1,:])\n",
    "        results.iloc[i,2] = confusion[0,0]/np.sum(confusion[0,:])\n",
    "        results.iloc[i,3] = precision_score(y_test, y_pred)\n",
    "        y_df = svm.decision_function(x_test)\n",
    "        results.iloc[i,4] = roc_auc_score(y_test, y_df) \n",
    "results"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "model_fitting 17.4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
